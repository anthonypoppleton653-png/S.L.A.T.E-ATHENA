# Architecture
<!-- Modified: 2026-02-07T14:00:00Z | Author: CLAUDE | Change: Embed themed system architecture diagram -->

SLATE creates an AI operations layer on your local hardware that bridges to GitHub's cloud infrastructure.

## System Overview

<div align="center">

![SLATE System Architecture](../assets/system-architecture.svg)

</div>

### Architecture Layers

| Layer | Components | Purpose |
|:------|:-----------|:--------|
| **Presentation** | Dashboard, CLI, VS Code Extension | User interfaces |
| **Orchestration** | Task Router, Workflow Dispatcher, Load Balancer | Coordination |
| **Execution** | GitHub Actions, Self-hosted Runner | GPU-enabled task execution |
| **AI Backends** | Ollama, Foundry Local, External APIs | LLM inference |
| **Infrastructure** | Message Broker, RAG Memory, GPU Scheduler, LLM Cache | Core services |

### Design Philosophy

<table>
<tr>
<td width="25%" align="center"><strong>ğŸ”„ Synchronized</strong><br><sub>Harmonious component interactions</sub></td>
<td width="25%" align="center"><strong>ğŸŒ± Living</strong><br><sub>Adaptive, responsive system</sub></td>
<td width="25%" align="center"><strong>ğŸ—ï¸ Architecture</strong><br><sub>Structured, geometric foundations</sub></td>
<td width="25%" align="center"><strong>âš¡ Evolution</strong><br><sub>Progressive enhancement</sub></td>
</tr>
</table>

## Core Components

### Message Broker (`message_broker.py`)

Handles inter-agent communication using pub/sub messaging.

```python
from slate import get_broker, create_event

broker = get_broker()

# Publish an event
event = create_event(
    event_type="task_completed",
    data={"task_id": "t001", "result": "success"}
)
broker.publish("tasks", event)

# Subscribe to events
def handle_event(event):
    print(f"Received: {event}")

broker.subscribe("tasks", handle_event)
```

**Features:**
- In-memory default (no Redis required)
- Optional Redis for distributed deployments
- Topic-based routing
- Event persistence

### RAG Memory (`rag_memory.py`)

Vector-based memory system using ChromaDB.

```python
from slate import get_memory_manager

memory = get_memory_manager()

# Store context
memory.store_short_term("user_preference", {"theme": "dark"})

# Retrieve with semantic search
results = memory.search("authentication flow", top_k=5)
```

**Memory Types:**
- **Short-term**: Session data, current context
- **Long-term**: Persistent knowledge, learned patterns
- **Episodic**: Task history, outcomes

### GPU Scheduler (`gpu_scheduler.py`)

Manages GPU workload distribution across devices.

```python
from slate import get_scheduler, ComputeType

scheduler = get_scheduler()

# Get optimal device for task
device = scheduler.get_device(
    compute_type=ComputeType.GPU_PREFERRED,
    memory_required_mb=4096
)

# Monitor utilization
stats = scheduler.get_stats()
print(f"GPU 0: {stats['gpu_0_utilization']}%")
```

**Features:**
- Multi-GPU support
- Memory-aware allocation
- Workload balancing
- Fallback to CPU

### LLM Cache (`llm_cache.py`)

Response caching to reduce redundant API calls.

```python
from slate import LLMCache

cache = LLMCache()

# Check cache before calling API
cached = cache.get(prompt_hash)
if cached:
    return cached

# Store response
cache.set(prompt_hash, response, ttl=3600)
```

**Features:**
- Content-addressed storage
- TTL-based expiration
- Disk persistence
- Hit rate metrics

## Data Flow

### Task Execution Flow

```
1. Task Created
       â”‚
       â–¼
2. Task Router
   - Analyzes complexity
   - Determines agent
   - Checks dependencies
       â”‚
       â–¼
3. Agent Scheduler
   - Selects available agent
   - Allocates resources
   - Queues if busy
       â”‚
       â–¼
4. Agent Execution
   - Retrieves context
   - Calls AI backend
   - Generates response
       â”‚
       â–¼
5. Result Processing
   - Validates output
   - Updates memory
   - Triggers events
       â”‚
       â–¼
6. Task Completed
```

### AI Backend Selection

```
Request
   â”‚
   â–¼
Unified Backend
   â”‚
   â”œâ”€â”€ Check Ollama â”€â”€â†’ Available? â”€â”€â†’ Use Ollama
   â”‚                         â”‚
   â”‚                         â–¼ No
   â”œâ”€â”€ Check Foundry â”€â†’ Available? â”€â”€â†’ Use Foundry
   â”‚                         â”‚
   â”‚                         â–¼ No
   â””â”€â”€ External APIs â”€â†’ Available? â”€â”€â†’ Use API
                             â”‚
                             â–¼ No
                        Error: No backend
```

## Module Dependencies

```
slate/
â”œâ”€â”€ Core (no dependencies)
â”‚   â”œâ”€â”€ message_broker.py
â”‚   â”œâ”€â”€ file_lock.py
â”‚   â””â”€â”€ llm_cache.py
â”‚
â”œâ”€â”€ Infrastructure (depends on Core)
â”‚   â”œâ”€â”€ rag_memory.py      â†’ message_broker
â”‚   â”œâ”€â”€ gpu_scheduler.py   â†’ file_lock
â”‚   â””â”€â”€ gpu_embeddings.py  â†’ rag_memory
â”‚
â”œâ”€â”€ AI Layer (depends on Infrastructure)
â”‚   â”œâ”€â”€ unified_ai_backend.py â†’ llm_cache, gpu_scheduler
â”‚   â”œâ”€â”€ ollama_client.py      â†’ llm_cache
â”‚   â””â”€â”€ foundry_local.py      â†’ llm_cache
â”‚
â”œâ”€â”€ Agent Layer (depends on AI)
â”‚   â”œâ”€â”€ slate_agent_v2.py â†’ unified_ai_backend, rag_memory
â”‚   â””â”€â”€ slate_orchestrator.py â†’ agent_v2
â”‚
â””â”€â”€ Tools (depends on various)
    â”œâ”€â”€ metrics_aggregator.py â†’ gpu_scheduler
    â”œâ”€â”€ load_balancer.py â†’ message_broker
    â””â”€â”€ feature_flags.py â†’ (standalone)
```

## GitHub Integration

SLATE bridges your local hardware to GitHub's cloud:

```
GitHub Issues â†’ SLATE local queue â†’ Local AI processes â†’ Results â†’ GitHub PRs/Comments
```

### Self-Hosted Runner

SLATE auto-configures a GitHub Actions runner with AI access:
- GPU labels auto-detected (cuda, multi-gpu, blackwell)
- Pre-job hooks set environment variables
- Workflows call local LLMs without external APIs

### Project Board Sync

| Board | Auto-Route Keywords |
|-------|---------------------|
| KANBAN | Default for pending |
| BUG TRACKING | bug, fix, crash, error |
| ROADMAP | feat, add, implement |
| PLANNING | plan, design, architect |

### Workflow Architecture

| Workflow | Trigger | Purpose |
|----------|---------|---------|
| CI Pipeline | Push/PR | Linting, tests, AI code review |
| AI Maintenance | Every 4h | Codebase analysis, auto-docs |
| Nightly Jobs | Daily 4am | Full test suite, dependency audit |
| Project Automation | Every 30min | Sync Issues/PRs to boards |

## Built-In Safeguards

### ActionGuard

Every action goes through validation:

```python
from slate.action_guard import validate_action

validate_action("rm -rf /")  # BLOCKED
validate_action("pip install pkg")  # Allowed
```

**Blocked patterns:**
- `rm -rf`, `format`, `del /s` - Destructive commands
- `0.0.0.0` bindings - Network exposure
- `eval()`, `exec()` - Dynamic execution
- External paid API calls

### SDK Source Guard

Only trusted publishers:
- Microsoft, NVIDIA, Meta, Google, Hugging Face
- Unknown PyPI packages blocked

### PII Scanner

Before GitHub sync:
- Scans for API keys, tokens, credentials
- Blocks sensitive data from public boards

### Network Isolation

All services bind to localhost only:

```python
host = "127.0.0.1"  # Never "0.0.0.0"
```

## Configuration Hierarchy

```
1. Environment Variables (highest priority)
   SLATE_OLLAMA_HOST, SLATE_LOG_LEVEL, etc.
       â”‚
       â–¼
2. Local Configuration
   .env, config/local.yaml
       â”‚
       â–¼
3. Project Configuration
   pyproject.toml, .specify/memory/constitution.md
       â”‚
       â–¼
4. Default Values (lowest priority)
   Hardcoded in modules
```

## Kubernetes & Container Architecture
<!-- Modified: 2026-02-09T04:30:00Z | Author: COPILOT | Change: Add K8s architecture section to wiki -->

SLATE runs as a **local cloud** using Kubernetes for production stability. The Docker release
image (`slate:local`) packages all SLATE runtimes into a single CUDA 12.8 container, which
Kubernetes then deploys and manages.

### Release Image

```
docker build -t slate:local .
```

The image includes: Core SDK, Dashboard, Agent Router, Autonomous Loop, Copilot Bridge,
Workflow Manager, ML Pipeline, GPU Manager, Security Guards, Models, and Skills.

### K8s Deployments

| Deployment | Replicas | Purpose |
|------------|----------|---------|
| `slate-core` | 2 | Core SDK + Dashboard (ports 8080-8084) |
| `ollama` | 1 | LLM inference with GPU access |
| `chromadb` | 1 | Vector store for RAG memory |
| `slate-agent-router` | 2 | Agent task routing |
| `slate-autonomous-loop` | 1 | Autonomous task execution |
| `slate-copilot-bridge` | 1 | Copilot â†” agent bridge |
| `slate-workflow-manager` | 1 | Task lifecycle management |

### K8s Resources

- **Namespace**: `slate`
- **HPAs**: Auto-scaling based on CPU/memory
- **PDBs**: Pod disruption budgets for availability
- **NetworkPolicies**: Namespace-scoped isolation
- **CronJobs**: ML training, codebase indexing, health checks
- **PVCs**: Persistent storage for models, ChromaDB, configs
- **RBAC**: Service accounts with scoped permissions

### Deployment Options

```bash
# Kustomize (recommended for local)
kubectl apply -k k8s/overlays/local/

# Helm
helm install slate ./helm -f helm/values.yaml

# Status
python slate/slate_k8s_deploy.py --status
python slate/slate_k8s_deploy.py --health
```

### Docker Compose (Alternative)

For simpler scenarios:
- **Dev:** `docker-compose -f docker-compose.dev.yml up`
- **Prod:** `docker-compose -f docker-compose.prod.yml up -d`

## Next Steps

- [Learn about Agents](Agents)
- [Configure AI Backends](AI-Backends)
- [CLI Reference](CLI-Reference)
- [Kubernetes Deployment](CLI-Reference#kubernetes--containers)
