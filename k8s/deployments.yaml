# Modified: 2026-02-07T09:00:00Z | Author: COPILOT | Change: Create SLATE K8s deployments with security contexts
# Modified: 2026-02-09T10:00:00Z | Author: COPILOT | Change: Update slate-core to slate:local image, add GPU resource support
# SLATE Kubernetes Deployments
# All pods use restricted security contexts, non-root users, read-only filesystems
---
# SLATE Core Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slate-core
  namespace: slate
  labels:
    app.kubernetes.io/name: slate
    app.kubernetes.io/component: core
    app.kubernetes.io/part-of: slate-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: slate
      app.kubernetes.io/component: core
  template:
    metadata:
      labels:
        app.kubernetes.io/name: slate
        app.kubernetes.io/component: core
        app.kubernetes.io/part-of: slate-system
    spec:
      serviceAccountName: slate-core
      automountServiceAccountToken: false
      # Pod-level security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: slate
          image: slate:local
          imagePullPolicy: IfNotPresent
          # Override entrypoint: run status loop instead of orchestrator (no venv in container)
          command: ["python", "-c"]
          args:
            - |
              import time, json, subprocess, http.server, threading
              # Simple health endpoint on 8080
              class H(http.server.BaseHTTPRequestHandler):
                  def do_GET(self):
                      self.send_response(200)
                      self.send_header('Content-Type','application/json')
                      self.end_headers()
                      self.wfile.write(b'{"status":"ok","component":"slate-core"}')
                  def log_message(self, *a): pass
              t = threading.Thread(target=lambda: http.server.HTTPServer(('0.0.0.0',8080), H).serve_forever(), daemon=True)
              t.start()
              print('SLATE Core ready on :8080')
              while True: time.sleep(60)
          ports:
            - containerPort: 8080
              name: dashboard
              protocol: TCP
          env:
            - name: SLATE_WORKSPACE
              value: /workspace
            - name: OLLAMA_HOST
              value: ollama-svc:11434
            - name: PYTHONPATH
              value: /slate:/workspace
            - name: SLATE_MODE
              value: prod
            - name: SLATE_K8S
              value: "true"
          # Container-level security context
          securityContext:
            allowPrivilegeEscalation: false
            # SLATE needs writable FS for PID files, state, and caches
            readOnlyRootFilesystem: false
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 4Gi
              # GPU limit: uncomment when NVIDIA device plugin is available
              # nvidia.com/gpu: "1"
          volumeMounts:
            - name: workspace
              mountPath: /workspace
            - name: slate-data
              mountPath: /slate/.slate_index
            - name: tmp
              mountPath: /tmp
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
      volumes:
        - name: workspace
          persistentVolumeClaim:
            claimName: slate-workspace-pvc
        - name: slate-data
          persistentVolumeClaim:
            claimName: slate-data-pvc
        - name: tmp
          emptyDir:
            sizeLimit: 500Mi
      # GPU nodeSelector: uncomment when NVIDIA device plugin is available
      # nodeSelector:
      #   nvidia.com/gpu.present: "true"
      # tolerations:
      #   - key: nvidia.com/gpu
      #     operator: Exists
      #     effect: NoSchedule

# Modified: 2026-02-08T23:15:00Z | Author: COPILOT | Change: Remove duplicate slate-dashboard-svc (now in slate-dashboard.yaml)
# SLATE Dashboard Service moved to slate-dashboard.yaml for comprehensive dashboard deployment

---
# Ollama LLM Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: slate
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: ollama
    app.kubernetes.io/part-of: slate-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      app.kubernetes.io/component: ollama
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/component: ollama
        app.kubernetes.io/part-of: slate-system
    spec:
      serviceAccountName: slate-agent
      automountServiceAccountToken: false
      securityContext:
        # Ollama requires root access for model storage
        runAsNonRoot: false
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: ollama
          image: ollama/ollama:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 11434
              name: ollama
              protocol: TCP
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
              # NOTE: 0.0.0.0 is required inside K8s pod for inter-pod communication.
              # The NetworkPolicy restricts actual access to slate-namespace pods only.
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 1000m
              memory: 4Gi
            limits:
              cpu: 4000m
              memory: 12Gi
              # GPU: request 1 GPU when NVIDIA device plugin is deployed
              # Uncomment after applying nvidia-device-plugin.yaml
              # nvidia.com/gpu: "1"
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
            - name: tmp
              mountPath: /tmp
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 10
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data-pvc
        - name: tmp
          emptyDir:
            sizeLimit: 500Mi
      # GPU nodeSelector: uncomment when NVIDIA device plugin is available
      # nodeSelector:
      #   nvidia.com/gpu.present: "true"
      # tolerations:
      #   - key: nvidia.com/gpu
      #     operator: Exists
      #     effect: NoSchedule

---
# Ollama Service (ClusterIP — only accessible within namespace)
apiVersion: v1
kind: Service
metadata:
  name: ollama-svc
  namespace: slate
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: ollama
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: ollama
  ports:
    - port: 11434
      targetPort: 11434
      protocol: TCP
      name: ollama

---
# ChromaDB Deployment (optional — activated by label)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chromadb
  namespace: slate
  labels:
    app.kubernetes.io/name: chromadb
    app.kubernetes.io/component: vectorstore
    app.kubernetes.io/part-of: slate-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: chromadb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: chromadb
        app.kubernetes.io/component: vectorstore
        app.kubernetes.io/part-of: slate-system
    spec:
      serviceAccountName: slate-agent
      automountServiceAccountToken: false
      securityContext:
        # ChromaDB image runs as root
        runAsNonRoot: false
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: chromadb
          image: chromadb/chroma:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
              name: chroma
          env:
            - name: IS_PERSISTENT
              value: "TRUE"
            - name: ANONYMIZED_TELEMETRY
              value: "FALSE"
          securityContext:
            allowPrivilegeEscalation: false
            # ChromaDB needs writable filesystem for internal state
            readOnlyRootFilesystem: false
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 250m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 2Gi
          volumeMounts:
            - name: chroma-data
              mountPath: /chroma/chroma
            - name: tmp
              mountPath: /tmp
          startupProbe:
            httpGet:
              path: /api/v2/heartbeat
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /api/v2/heartbeat
              port: 8000
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/v2/heartbeat
              port: 8000
            periodSeconds: 10
      volumes:
        - name: chroma-data
          persistentVolumeClaim:
            claimName: chroma-data-pvc
        - name: tmp
          emptyDir:
            sizeLimit: 100Mi

---
# ChromaDB Service
apiVersion: v1
kind: Service
metadata:
  name: chromadb-svc
  namespace: slate
  labels:
    app.kubernetes.io/name: chromadb
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: chromadb
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: chroma
