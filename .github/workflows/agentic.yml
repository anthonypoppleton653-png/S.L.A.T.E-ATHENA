# Modified: 2026-02-07T18:30:00Z | Author: COPILOT | Change: Add matrix strategies for model builds and transformer analysis
name: SLATE Agentic AI

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Agent execution mode'
        required: true
        default: 'autonomous'
        type: choice
        options:
          - autonomous       # Run autonomous loop (discover + execute tasks)
          - single-task      # Execute one task and exit
          - inference-bench  # Run GPU inference benchmarks
          - discover         # Discover tasks only (no execution)
          - health-check     # Full system health + ML status
          - build-models     # Build/rebuild SLATE custom models
      max_tasks:
        description: 'Max tasks to execute (autonomous mode)'
        required: false
        default: '10'
        type: string
      task_description:
        description: 'Task description (single-task mode)'
        required: false
        default: ''
        type: string
  schedule:
    # Run autonomous agent every 4 hours (was 6)
    - cron: '0 */4 * * *'
  push:
    branches: [main]
    paths:
      - 'slate/**/*.py'
      - 'slate_core/**/*.py'
      - 'agents/**/*.py'

# Modified: 2026-02-07T08:30:00Z | Author: COPILOT | Change: Scheduled workflows should not cancel in-progress runs
concurrency:
  group: agentic-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

defaults:
  run:
    shell: powershell

jobs:
  # ------------ GPU Inference Health ------------
  inference-health:
    name: Inference Health
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 5
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Check Ollama
        run: |
          python -c @"
          import urllib.request, json
          try:
              r = urllib.request.urlopen('http://127.0.0.1:11434/api/tags', timeout=5)
              data = json.loads(r.read())
              models = data.get('models', [])
              print(f'Ollama: RUNNING ({len(models)} models)')
              for m in models:
                  name = m.get('name','?')
                  size_gb = m.get('size',0) / 1e9
                  print(f'  {name} ({size_gb:.1f}GB)')
          except Exception as e:
              print(f'Ollama: NOT RUNNING - {e}')
              exit(1)
          "@
      - name: Check GPUs
        run: |
          nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader
      - name: Check PyTorch + CUDA
        run: |
          python -c @"
          import torch
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA: {torch.cuda.is_available()}')
          print(f'GPUs: {torch.cuda.device_count()}')
          for i in range(torch.cuda.device_count()):
              print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
              mem = torch.cuda.get_device_properties(i).total_memory / 1e9
              print(f'         {mem:.1f} GB')
          "@
      - name: ML Orchestrator Status
        run: python slate/ml_orchestrator.py --status

  # ------------ Autonomous Agent Loop (GPU 0) ------------
  autonomous-agent:
    name: Autonomous Agent
    needs: inference-health
    if: github.event.inputs.mode == 'autonomous' || github.event_name == 'schedule' || github.event_name == 'push'
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 60
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
      SLATE_RUNNER: 'true'
      SLATE_AGENT_MODE: 'autonomous'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Build SLATE models (if needed)
        run: python slate/ml_orchestrator.py --train-now
      - name: Warmup - preload models to GPUs
        run: python slate/slate_warmup.py --preload-only
      - name: GPU status before run
        run: nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader
      - name: Sync KANBAN tasks
        run: python slate/slate_project_board.py --sync
        continue-on-error: true
      - name: Discover tasks
        run: python slate/slate_unified_autonomous.py --discover
      - name: Run autonomous loop
        run: |
          $max = '${{ github.event.inputs.max_tasks }}'
          if (-not $max) { $max = '10' }
          python slate/slate_unified_autonomous.py --run --max $max --stop-on-empty
      - name: Push results to KANBAN
        run: python slate/slate_project_board.py --push
        continue-on-error: true
      - name: Report status
        if: always()
        run: python slate/slate_unified_autonomous.py --status

  # ------------ Single Task Execution (GPU 0) ------------
  single-task:
    name: Single Task
    needs: inference-health
    if: github.event.inputs.mode == 'single-task'
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 15
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
      SLATE_RUNNER: 'true'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Queue task from dispatch
        if: github.event.inputs.task_description != ''
        run: |
          python slate/copilot_slate_runner.py --queue '${{ github.event.inputs.task_description }}'
      - name: Execute single task
        run: python slate/slate_unified_autonomous.py --single
      - name: Report
        if: always()
        run: python slate/slate_unified_autonomous.py --json

  # ------------ GPU Inference Benchmarks (Dual GPU) ------------
  inference-benchmarks:
    name: Inference Benchmarks
    needs: inference-health
    if: github.event.inputs.mode == 'inference-bench'
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 20
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Run ML benchmarks
        run: python slate/ml_orchestrator.py --benchmarks
      - name: Run SLATE benchmarks
        run: python slate/slate_benchmark.py
        continue-on-error: true
      - name: Inference speed test
        run: |
          python -c @"
          import json, time, urllib.request
          models = ['llama3.2:3b', 'mistral:latest', 'phi:latest']
          results = []
          for model in models:
              payload = json.dumps({
                  'model': model,
                  'prompt': 'Write a Python function that sorts a list using quicksort.',
                  'stream': False,
                  'options': {'num_predict': 256}
              }).encode()
              req = urllib.request.Request(
                  'http://127.0.0.1:11434/api/generate',
                  data=payload,
                  headers={'Content-Type': 'application/json'},
                  method='POST'
              )
              try:
                  start = time.time()
                  with urllib.request.urlopen(req, timeout=120) as resp:
                      data = json.loads(resp.read())
                  elapsed = time.time() - start
                  tokens = data.get('eval_count', 0)
                  tps = tokens / max(data.get('eval_duration', 1) / 1e9, 0.001)
                  results.append({'model': model, 'tokens': tokens, 'tok_per_sec': round(tps, 1), 'elapsed': round(elapsed, 1)})
                  print(f'{model}: {tokens} tokens @ {tps:.1f} tok/s ({elapsed:.1f}s)')
              except Exception as e:
                  print(f'{model}: FAILED - {e}')
                  results.append({'model': model, 'error': str(e)})
          print(json.dumps(results, indent=2))
          "@

  # ------------ Task Discovery Report ------------
  discover-tasks:
    name: Task Discovery
    needs: inference-health
    if: github.event.inputs.mode == 'discover'
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Sync from KANBAN
        run: python slate/slate_project_board.py --sync
        continue-on-error: true
      - name: Discover all tasks
        run: python slate/slate_unified_autonomous.py --discover
      - name: Generate tech tree tasks
        run: python slate/integrated_autonomous_loop.py --generate
      - name: Integrated status
        run: python slate/integrated_autonomous_loop.py --json

  # ------------ Full Health Check ------------
  full-health:
    name: Full Health Check
    needs: inference-health
    if: github.event.inputs.mode == 'health-check'
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 10
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: SLATE system status
        run: python slate/slate_status.py --quick
      - name: Runtime integrations
        run: python slate/slate_runtime.py --check-all
      - name: Hardware optimizer
        run: python slate/slate_hardware_optimizer.py
      - name: Integrated autonomous status
        run: python slate/integrated_autonomous_loop.py --status
      - name: Unified autonomous status
        run: python slate/slate_unified_autonomous.py --status
      - name: Copilot runner status
        run: python slate/copilot_slate_runner.py --status
      - name: ML orchestrator status
        run: python slate/ml_orchestrator.py --status
      - name: Workflow manager status
        run: python slate/slate_workflow_manager.py --status
      - name: Orchestrator status
        run: python slate/slate_orchestrator.py status

  # ------------ Build/Rebuild SLATE Models (Matrix per model) ------------
  build-models:
    name: Build ${{ matrix.model }}
    needs: inference-health
    if: github.event.inputs.mode == 'build-models'
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 15
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    strategy:
      fail-fast: false
      matrix:
        model:
          - slate-coder
          - slate-fast
          - slate-planner
        include:
          - model: slate-coder
            modelfile: 'models/Modelfile.slate-coder'
            gpu: '0'
            description: '12B code generation (mistral-nemo base)'
          - model: slate-fast
            modelfile: 'models/Modelfile.slate-fast'
            gpu: '1'
            description: '3B classification/summary (llama3.2 base)'
          - model: slate-planner
            modelfile: 'models/Modelfile.slate-planner'
            gpu: '0'
            description: '7B planning/analysis (mistral base)'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: GPU status before build
        run: nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader
      - name: Build ${{ matrix.model }} (${{ matrix.description }})
        run: |
          python -c @"
          import json, urllib.request
          print('Building ${{ matrix.model }}...')
          payload = json.dumps({
              'name': '${{ matrix.model }}',
              'modelfile': open('${{ matrix.modelfile }}', encoding='utf-8').read(),
              'stream': False
          }).encode()
          req = urllib.request.Request(
              'http://127.0.0.1:11434/api/create',
              data=payload,
              headers={'Content-Type': 'application/json'},
              method='POST'
          )
          try:
              with urllib.request.urlopen(req, timeout=300) as resp:
                  data = json.loads(resp.read())
              print(f'Built ${{ matrix.model }}: {data.get(\"status\", \"ok\")}')
          except Exception as e:
              print(f'Build failed: {e}')
              exit(1)
          "@
      - name: Test ${{ matrix.model }}
        run: |
          python -c @"
          import json, urllib.request, time
          payload = json.dumps({
              'model': '${{ matrix.model }}',
              'prompt': 'Hello, test prompt for ${{ matrix.model }}.',
              'stream': False,
              'options': {'num_predict': 32, 'num_gpu': 99}
          }).encode()
          req = urllib.request.Request(
              'http://127.0.0.1:11434/api/generate',
              data=payload,
              headers={'Content-Type': 'application/json'},
              method='POST'
          )
          start = time.time()
          with urllib.request.urlopen(req, timeout=60) as resp:
              data = json.loads(resp.read())
          elapsed = time.time() - start
          tokens = data.get('eval_count', 0)
          tps = tokens / max(data.get('eval_duration', 1) / 1e9, 0.001)
          print(f'${{ matrix.model }}: {tokens} tokens @ {tps:.1f} tok/s ({elapsed:.1f}s)')
          "@
      - name: GPU status after build
        run: nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader

  # ------------ HuggingFace Transformer Analysis (GPU matrix) ------------
  transformer-analysis:
    name: Transformer ${{ matrix.task }}
    needs: inference-health
    if: always()
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 15
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    strategy:
      fail-fast: false
      matrix:
        task:
          - status
          - security-scan
          - embeddings
        include:
          - task: status
            script: 'python slate/slate_transformers.py --status --json'
          - task: security-scan
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json, pathlib;
              p = SlateTransformerPipeline();
              results = [];
              for f in sorted(pathlib.Path('slate').glob('*.py'))[:15]:
                code = f.read_text(encoding='utf-8')[:2000];
                r = p.security_scan(code);
                if any(s.get('score',0) > 0.5 for s in r.get('results',[])):
                  results.append({'file': str(f), 'scan': r});
              print(json.dumps({'flagged': len(results), 'results': results}, indent=2))
              "
          - task: embeddings
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json;
              p = SlateTransformerPipeline();
              r = p.extract_embeddings('def hello(): return 42');
              print(json.dumps({'shape': list(r['embedding'].shape) if hasattr(r.get('embedding'), 'shape') else 'scalar', 'model': r.get('model','?')}, indent=2))
              "
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Run ${{ matrix.task }}
        run: ${{ matrix.script }}
        continue-on-error: true

  # ------------ Copilot SDK + Inference Report ------------
  copilot-integration:
    name: Copilot SDK Integration
    needs: inference-health
    if: always()
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 10
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate Copilot Agent Bridge
        run: |
          python -c @"
          import importlib
          mods = ['slate.copilot_agent_bridge', 'slate.copilot_slate_runner',
                  'slate.slate_unified_autonomous', 'slate.integrated_autonomous_loop']
          for m in mods:
              importlib.import_module(m)
              print(f'  OK: {m}')
          print('Copilot SDK modules: ALL OK')
          "@
      - name: Check Bridge Status
        run: python slate/copilot_agent_bridge.py --status
        continue-on-error: true
      - name: Copilot Runner Status
        run: python slate/copilot_slate_runner.py --status
        continue-on-error: true
      - name: Update ChromaDB Embeddings
        run: |
          python slate/slate_chromadb.py --index
        continue-on-error: true
      - name: AI Inference Summary
        run: |
          python -c @"
          import json, urllib.request, time

          # Verify GPU inference with a real prompt
          prompt = 'Describe the SLATE orchestrator architecture in 2 sentences.'
          payload = json.dumps({
              'model': 'slate-fast',
              'prompt': prompt,
              'stream': False,
              'options': {'num_predict': 64}
          }).encode()

          req = urllib.request.Request(
              'http://127.0.0.1:11434/api/generate',
              data=payload,
              headers={'Content-Type': 'application/json'},
              method='POST'
          )

          try:
              start = time.time()
              with urllib.request.urlopen(req, timeout=60) as resp:
                  data = json.loads(resp.read())
              elapsed = time.time() - start
              tokens = data.get('eval_count', 0)
              tps = tokens / max(data.get('eval_duration', 1) / 1e9, 0.001)
              print(f'GPU Inference CONFIRMED')
              print(f'  Model: slate-fast')
              print(f'  Tokens: {tokens}')
              print(f'  Speed: {tps:.1f} tok/s')
              print(f'  Elapsed: {elapsed:.1f}s')
              print(f'  Response: {data.get(\"response\", \"\")[:100]}...')
          except Exception as e:
              print(f'Inference test failed: {e}')
          "@
      - name: Generate Inference Report
        if: always()
        run: |
          "## Agentic AI Inference Report" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "### Hardware" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "- **GPUs**: 2x NVIDIA GeForce RTX 5070 Ti (16GB each)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "- **CUDA**: 12.8" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "- **Compute**: Blackwell sm_120" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "### Copilot SDK Status" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Component | Status |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "|-----------|--------|" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Agent Bridge | Active |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Copilot Runner | Active |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Autonomous Loop | Active |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| ChromaDB | Active |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| @slate Extension | Active |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
