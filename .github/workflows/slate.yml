# Modified: 2026-02-07T15:45:00Z | Author: COPILOT | Change: Add AI inference validation to integration tests
name: SLATE Integration

on:
  push:
    branches: [main, develop, 'feature/*']
  workflow_dispatch:

concurrency:
  group: slate-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

defaults:
  run:
    shell: powershell

jobs:
  slate-status:
    name: SLATE Status Check
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: SLATE Status
        run: python slate/slate_status.py --quick

  sdk-validation:
    name: SDK Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Import all modules
        run: |
          python -c @"
          import importlib, pathlib, sys
          ok, fail = [], []
          for f in sorted(pathlib.Path('slate').glob('*.py')):
              mod = f.stem
              if mod.startswith('_'): continue
              try:
                  importlib.import_module(f'slate.{mod}')
                  ok.append(mod)
                  print(f'  OK: slate.{mod}')
              except Exception as e:
                  fail.append((mod, str(e)))
                  print(f'  FAIL: slate.{mod}: {e}')
          print(f'Audit: {len(ok)} OK, {len(fail)} failed')
          if fail: sys.exit(1)
          "@

  tech-tree:
    name: Tech Tree Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate tech tree
        run: |
          python -c @"
          import json, pathlib
          p = pathlib.Path('.slate_tech_tree/tech_tree.json')
          if p.exists():
              tree = json.loads(p.read_text())
              nodes = tree.get('nodes', [])
              total = len(nodes)
              complete = sum(1 for n in nodes if n.get('status') == 'complete')
              print(f'Tech Tree: {complete}/{total} nodes complete')
          else:
              print('No tech tree found')
          "@

  dashboard:
    name: Dashboard Smoke Test
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Test dashboard import
        run: |
          python -c "from agents.slate_dashboard_server import create_app; print('Dashboard import OK')"
        continue-on-error: true

  agents:
    name: Agent Module Test
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Test agent imports
        run: |
          python -c "import agents; print('agents package OK')"
          python -c "from agents.runner_api import RunnerAPI; print('runner_api OK')"
          python -c "from agents.install_api import InstallAPI; print('install_api OK')"
        continue-on-error: true

  slate-agent:
    name: SLATE Agent Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate agent definition
        run: |
          python -c @"
          # Modified: 2026-02-07T03:00:00Z | Author: COPILOT | Change: CI validation for @slate agent
          import pathlib, re, sys

          agent = pathlib.Path('.github/agents/slate.agent.md')
          if not agent.exists():
              print('SKIP: No agent file found')
              sys.exit(0)

          content = agent.read_text(encoding='utf-8')
          parts = content.split('---', 2)
          assert len(parts) >= 3, 'Agent frontmatter incomplete'

          # Validate frontmatter fields
          fm = parts[1]
          assert 'name: slate' in fm, 'Agent name must be slate'
          assert 'description:' in fm, 'Missing description'
          assert 'tools:' in fm, 'Missing tools list'
          print('Frontmatter: OK')

          # Validate required sections
          body = parts[2]
          for section in ['Protocol Commands', 'Behavior Rules', 'Agent Routing', 'Project Structure', 'Handling Requests']:
              assert section in body, f'Missing section: {section}'
              print(f'  Section: {section} OK')

          # Validate referenced SLATE scripts exist
          scripts = set(re.findall(r'slate/(\w+\.py)', body))
          missing = [s for s in scripts if not (pathlib.Path('slate') / s).exists()]
          print(f'Scripts: {len(scripts)} referenced, {len(missing)} missing')
          assert not missing, f'Missing scripts: {missing}'

          # Validate SLATE protocol commands are executable
          import importlib
          for mod in ['slate_status', 'slate_runtime', 'slate_runner_manager', 'slate_hardware_optimizer', 'slate_orchestrator', 'slate_workflow_manager', 'slate_benchmark']:
              importlib.import_module(f'slate.{mod}')
              print(f'  Import slate.{mod}: OK')

          print('SLATE Agent Validation: PASSED')
          "@
      - name: Smoke test SLATE protocols
        run: |
          python slate/slate_status.py --quick
          python slate/slate_runtime.py --check-all

  extension-build:
    name: Extension Build Check
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Check Node.js
        run: |
          node --version
          npm --version
      - name: Install extension deps
        run: |
          Set-Location plugins/slate-copilot
          npm install --ignore-scripts
      - name: Compile extension
        run: |
          Set-Location plugins/slate-copilot
          npm run compile
      - name: Verify outputs
        run: |
          $files = @(
            'plugins/slate-copilot/out/extension.js',
            'plugins/slate-copilot/out/slateParticipant.js',
            'plugins/slate-copilot/out/tools.js',
            'plugins/slate-copilot/out/slateRunner.js'
          )
          foreach ($f in $files) {
            if (Test-Path $f) { Write-Host "  OK: $f" }
            else { Write-Host "  FAIL: $f"; exit 1 }
          }
          Write-Host 'Extension build: PASSED'

  task-queue:
    name: Task Queue Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate task queue
        run: |
          python -c @"
          import json, pathlib
          p = pathlib.Path('current_tasks.json')
          if p.exists():
              data = json.loads(p.read_text())
              tasks = data.get('tasks', data) if isinstance(data, dict) else data
              if isinstance(tasks, list):
                  print(f'Task queue: {len(tasks)} tasks')
                  for t in tasks[:5]:
                      status = t.get('status', 'unknown')
                      title = t.get('title', t.get('name', 'unnamed'))
                      print(f'  [{status}] {title}')
              else:
                  print('Task queue format OK')
          else:
              print('No task queue found')
          "@

  summary:
    name: SLATE Summary
    runs-on: [self-hosted, slate]
    needs: [slate-status, sdk-validation, tech-tree, dashboard, agents, slate-agent, extension-build, task-queue, ai-inference-validation]
    if: always()
    steps:
      - name: Generate summary
        run: |
          "## SLATE Integration Summary" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Check | Status |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "|-------|--------|" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SLATE Status | ${{ needs.slate-status.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SDK Validation | ${{ needs.sdk-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Tech Tree | ${{ needs.tech-tree.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Dashboard | ${{ needs.dashboard.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Agents | ${{ needs.agents.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SLATE Agent | ${{ needs.slate-agent.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Extension Build | ${{ needs.extension-build.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Task Queue | ${{ needs.task-queue.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| AI Inference | ${{ needs.ai-inference-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY

  # ------------ AI Inference Validation (GPU) ------------
  ai-inference-validation:
    name: AI Inference Validation
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 15
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Verify GPU + Ollama
        run: |
          nvidia-smi --query-gpu=index,name,memory.used,utilization.gpu --format=csv,noheader
          python -c @"
          import torch
          assert torch.cuda.is_available(), 'CUDA not available'
          assert torch.cuda.device_count() >= 2, f'Expected 2 GPUs, got {torch.cuda.device_count()}'
          print(f'PyTorch {torch.__version__}, CUDA {torch.version.cuda}, {torch.cuda.device_count()} GPUs')
          "@
      - name: Verify Ollama Models
        id: ollama
        run: |
          $response = Invoke-RestMethod -Uri "http://127.0.0.1:11434/api/tags" -TimeoutSec 10
          $slateModels = $response.models | Where-Object { $_.name -like 'slate-*' }
          Write-Host "Ollama models: $($response.models.Count) total, $($slateModels.Count) SLATE custom"
          foreach ($m in $slateModels) { Write-Host "  $($m.name)" }

          if ($slateModels.Count -ge 3) {
            "slate_models=true" | Out-File -Append $env:GITHUB_OUTPUT
          } else {
            "slate_models=false" | Out-File -Append $env:GITHUB_OUTPUT
          }
      - name: Run Inference Test
        run: |
          python -c @"
          import json, urllib.request, time

          models_to_test = ['slate-fast', 'slate-coder', 'slate-planner']
          results = []
          for model in models_to_test:
              payload = json.dumps({
                  'model': model,
                  'prompt': 'What is SLATE?',
                  'stream': False,
                  'options': {'num_predict': 32}
              }).encode()

              req = urllib.request.Request(
                  'http://127.0.0.1:11434/api/generate',
                  data=payload,
                  headers={'Content-Type': 'application/json'}
              )
              try:
                  start = time.time()
                  with urllib.request.urlopen(req, timeout=60) as resp:
                      data = json.loads(resp.read())
                  elapsed = time.time() - start
                  tokens = data.get('eval_count', 0)
                  tps = tokens / max(data.get('eval_duration', 1) / 1e9, 0.001)
                  print(f'  {model}: {tokens} tokens @ {tps:.1f} tok/s ({elapsed:.1f}s)')
                  results.append({'model': model, 'tokens': tokens, 'tps': round(tps, 1)})
              except Exception as e:
                  print(f'  {model}: FAILED - {e}')
                  results.append({'model': model, 'error': str(e)})

          print(f'\\nInference test: {len([r for r in results if \"tokens\" in r])}/{len(models_to_test)} models OK')
          "@
      - name: ML Orchestrator Status
        run: python slate/ml_orchestrator.py --status
      - name: ChromaDB Status
        run: python slate/slate_chromadb.py --status
        continue-on-error: true
