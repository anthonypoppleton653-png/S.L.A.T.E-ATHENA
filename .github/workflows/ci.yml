# Modified: 2026-02-08T15:00:00Z | Author: COPILOT | Change: Mark as primary GitHub Actions pipeline per https://github.com/features/actions
# Modified: 2026-02-07T18:00:00Z | Author: COPILOT | Change: Add matrix strategies for unit tests and SDK modules
#
# SLATE CI — Primary GitHub Actions Pipeline
# This is the entry point for all code quality validation.
# Reference: https://github.com/features/actions
#
# Pipeline Integration:
#   CI (this) → stable-release.yml → CD → Docker → Deploy
#   CI runs on EVERY push/PR. Stable release runs on validated code only.
#
# Local Runner Requirement:
#   All jobs run on self-hosted [self-hosted, slate] runners with:
#   - 2x NVIDIA RTX 5070 Ti (16GB each)
#   - Local Ollama for AI-powered code review
#   - SLATE SDK validation
name: SLATE CI

on:
  push:
    branches: [main, develop, 'feature/*']
    paths-ignore: ['*.md', 'docs/**', '.github/*.md', 'specs/**']
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

# Modified: 2026-02-07T08:30:00Z | Author: COPILOT | Change: Fix concurrency group for PR-triggered workflows
concurrency:
  group: ci-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

defaults:
  run:
    shell: powershell

jobs:
  lint:
    name: Lint & Format
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Ruff lint
        run: |
          pip install ruff --quiet 2>$null
          ruff check slate/ agents/ --output-format=github
        continue-on-error: true
      - name: Ruff format check
        run: ruff format --check slate/ agents/ --diff
        continue-on-error: true

  unit-tests:
    name: Tests (${{ matrix.suite }})
    runs-on: [self-hosted, slate]
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        suite:
          - core
          - security
          - agents
          - ml
        include:
          - suite: core
            pattern: 'test_slate_runtime or test_slate_workflow or test_feature_flags or test_install_tracker or test_slate_benchmark or test_watcher or test_module_registry or test_slate_terminal_monitor or test_runner_cost_tracker or test_runner_fallback'
          - suite: security
            pattern: 'test_pii_scanner or test_action_guard or test_sdk_source_guard or test_slate_workflow_analyzer'
          - suite: agents
            pattern: 'test_agent_registry or test_agent_plugins or test_slate_runner_manager or test_slate_runner_benchmark or test_slate_real_multi_runner or test_copilot_slate_runner or test_slate_discussion_manager or test_slate_project_board'
          - suite: ml
            pattern: 'test_ml_orchestrator or test_slate_model_trainer or test_slate_gpu_manager or test_slate_warmup or test_slate_unified_autonomous or test_integrated_autonomous_loop or test_slate_hardware_optimizer'
          - suite: semantic-kernel
            pattern: 'test_slate_semantic_kernel'
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Install test deps
        run: pip install pytest pytest-cov pytest-asyncio --quiet 2>$null
        continue-on-error: true
      - name: Run ${{ matrix.suite }} tests
        run: python -m pytest tests/ -v --tb=short -x -k "${{ matrix.pattern }}"
        continue-on-error: true

  sdk-validation:
    name: SDK Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Verify SDK imports
        run: |
          python -c "import slate; print(f'SDK v{slate.__version__}')"
          python -c "import slate.slate_status; print('slate_status OK')"
          python -c "import slate.slate_runtime; print('slate_runtime OK')"
          python -c "import slate.slate_hardware_optimizer; print('hardware_optimizer OK')"
      - name: Version sync
        run: |
          python -c @"
          import tomllib
          with open('pyproject.toml', 'rb') as f:
              c = tomllib.load(f)
          v = c['project']['version']
          import slate
          assert slate.__version__ == v, f'Mismatch: {slate.__version__} != {v}'
          print(f'Version sync OK: {v}')
          "@

  security:
    name: Security Scan
    runs-on: [self-hosted, slate]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Bandit scan
        run: |
          pip install bandit --quiet 2>$null
          bandit -r slate/ agents/ -ll -ii -x "**/test*"
        continue-on-error: true

  slate-checks:
    name: SLATE Quick Checks
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate Tech Tree
        run: |
          python -c @"
          import json
          with open('.slate_tech_tree/tech_tree.json', 'r', encoding='utf-8') as f:
              tree = json.load(f)
          nodes = tree.get('nodes', [])
          completed = sum(1 for n in nodes if n.get('status') == 'complete')
          print(f'Tech Tree: {completed}/{len(nodes)} complete')
          "@
      - name: Validate Task Queue
        run: |
          python -c @"
          import json
          with open('current_tasks.json', 'r', encoding='utf-8') as f:
              data = json.load(f)
          tasks = data.get('tasks', data) if isinstance(data, dict) else data
          if isinstance(tasks, list):
              pending = sum(1 for t in tasks if t.get('status') == 'pending')
              print(f'Task Queue: {pending} pending tasks')
          else:
              print('Task queue OK')
          "@
      - name: Validate pyproject.toml
        run: |
          python -c @"
          import tomllib
          with open('pyproject.toml', 'rb') as f:
              config = tomllib.load(f)
          name = config['project']['name']
          ver = config['project']['version']
          print(f'Project: {name} v{ver}')
          "@

  gpu-validation:
    name: GPU Compute Validation
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 10
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: CUDA Environment Check
        run: |
          nvidia-smi --query-gpu=index,name,driver_version,memory.total --format=csv
      - name: GPU Compute Validation
        run: |
          python tests/test_gpu_compute.py

  # Modified: 2026-02-09T04:30:00Z | Author: COPILOT | Change: Add K8s manifest validation job
  k8s-validation:
    name: K8s Manifest Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate YAML manifests
        run: |
          python -c @"
          import yaml, pathlib, sys
          errors = 0
          for f in pathlib.Path('k8s').rglob('*.yaml'):
              try:
                  list(yaml.safe_load_all(f.read_text(encoding='utf-8')))
                  print(f'  OK  {f}')
              except Exception as e:
                  print(f'  FAIL {f}: {e}')
                  errors += 1
          print(f'\\nValidated {len(list(pathlib.Path("k8s").rglob("*.yaml")))} manifests, {errors} errors')
          sys.exit(1 if errors else 0)
          "@
      - name: Validate Helm chart
        run: |
          $helmPath = "$env:GITHUB_WORKSPACE\tools\helm.exe"
          if (Test-Path $helmPath) {
            & $helmPath lint helm/ --strict
          } else {
            Write-Host "::warning::Helm not found, skipping chart validation"
          }
        continue-on-error: true
      - name: K8s cluster connectivity
        run: |
          kubectl cluster-info 2>$null
          if ($LASTEXITCODE -eq 0) {
            python slate/slate_k8s_deploy.py --health
          } else {
            Write-Host "::warning::No K8s cluster available, skipping health check"
          }
        continue-on-error: true

  ai-code-review:
    name: AI Code Review
    runs-on: [self-hosted, slate]
    if: github.event_name == 'pull_request'
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Check Ollama
        id: ollama
        run: |
          $available = $false
          try {
            $response = Invoke-RestMethod -Uri "http://127.0.0.1:11434/api/tags" -TimeoutSec 5
            if ($response) { $available = $true }
          } catch { }
          "available=$available" | Out-File -Append $env:GITHUB_OUTPUT
      - name: AI Review Changed Files
        if: steps.ollama.outputs.available == 'True'
        run: |
          Write-Host "AI-powered code review for PR #${{ github.event.pull_request.number }}"
          Write-Host ""

          # Analyze recently changed files
          python slate/slate_ai_orchestrator.py --analyze-recent --json > ai_review.json

          $review = Get-Content ai_review.json -Raw | ConvertFrom-Json
          Write-Host "Files analyzed: $($review.files_analyzed)"
          Write-Host "Issues found: $($review.issues_found)"

          # Add review summary
          "## AI Code Review" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "**Files Analyzed**: $($review.files_analyzed)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "**Issues Found**: $($review.issues_found)" | Out-File -Append $env:GITHUB_STEP_SUMMARY

  # ------------ AI Inference on Code Changes (GPU) ------------
  ai-inference:
    name: AI Code Inference
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 20
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Verify Ollama + GPU
        id: ollama
        run: |
          $available = $false
          try {
            $response = Invoke-RestMethod -Uri "http://127.0.0.1:11434/api/tags" -TimeoutSec 5
            if ($response) {
              $available = $true
              $models = $response.models.name -join ','
              Write-Host "Ollama: RUNNING ($($response.models.Count) models)"
            }
          } catch { Write-Host "Ollama: not available" }
          "available=$available" | Out-File -Append $env:GITHUB_OUTPUT
      - name: GPU Status
        run: nvidia-smi --query-gpu=index,name,memory.used,utilization.gpu --format=csv,noheader
      - name: AI Analyze Changed Files
        if: steps.ollama.outputs.available == 'True'
        run: |
          Write-Host ""
          Write-Host "═══════════════════════════════════════════════════════════════"
          Write-Host "  AI Inference on Changed Files (Local GPU)"
          Write-Host "═══════════════════════════════════════════════════════════════"
          Write-Host ""

          # Run AI analysis on recently changed files
          python slate/slate_ai_orchestrator.py --analyze-recent --json > ai_inference.json

          $result = Get-Content ai_inference.json -Raw | ConvertFrom-Json -ErrorAction SilentlyContinue
          if ($result) {
            Write-Host "Files analyzed: $($result.files_analyzed)"
            Write-Host "Issues found: $($result.issues_found)"

            "## AI Code Inference (GPU)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
            "**Files Analyzed**: $($result.files_analyzed)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
            "**Issues Found**: $($result.issues_found)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
            "**GPU**: NVIDIA RTX 5070 Ti x2" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          }
      - name: Update ChromaDB Index
        if: steps.ollama.outputs.available == 'True' && github.event_name == 'push'
        run: |
          python slate/slate_chromadb.py --index
        continue-on-error: true
      - name: Upload AI Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-inference-${{ github.run_number }}
          path: ai_inference.json
          retention-days: 30
          if-no-files-found: ignore

  # Modified: 2026-02-08T22:00:00Z | Author: COPILOT | Change: Add Semantic Kernel validation job
  # ------------ Semantic Kernel Validation ------------
  semantic-kernel:
    name: Semantic Kernel Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Verify SK Import
        run: |
          python -c @"
          import semantic_kernel
          print(f'Semantic Kernel: v{semantic_kernel.__version__}')
          assert int(semantic_kernel.__version__.split('.')[0]) >= 1
          print('SK import: PASSED')
          "@
      - name: Verify SK Module
        run: |
          python -c @"
          from slate.slate_semantic_kernel import get_sk_status
          import json
          status = get_sk_status()
          print(json.dumps(status, indent=2))
          assert status['semantic_kernel']['available'], 'SK not available'
          assert len(status['plugins']) >= 3, 'Missing SK plugins'
          assert 'ActionGuard' in status['security'], 'ActionGuard not enforced'
          print('SK module validation: PASSED')
          "@
      - name: Verify SK Plugins
        run: |
          python -c @"
          from semantic_kernel import Kernel
          from slate.slate_semantic_kernel import _register_slate_plugins
          kernel = Kernel()
          _register_slate_plugins(kernel)
          plugin_names = list(kernel.plugins.keys())
          assert 'slate_system' in plugin_names, 'Missing slate_system plugin'
          assert 'slate_search' in plugin_names, 'Missing slate_search plugin'
          assert 'slate_agents' in plugin_names, 'Missing slate_agents plugin'
          total_funcs = sum(len(p) for p in kernel.plugins.values())
          print(f'SK Plugins: {len(plugin_names)} plugins, {total_funcs} functions')
          print('SK plugin validation: PASSED')
          "@
      - name: SK Model Resolution
        run: |
          python -c @"
          from slate.slate_semantic_kernel import _resolve_model, SK_MODEL_MAP
          for role in SK_MODEL_MAP:
              model = _resolve_model(role)
              print(f'  {role:12s} -> {model}')
          print('SK model resolution: PASSED')
          "@
      - name: SK Status Summary
        run: python slate/slate_semantic_kernel.py --status

  # ------------ Copilot SDK Validation ------------
  copilot-sdk:
    name: Copilot SDK Integration
    runs-on: [self-hosted, slate]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate Copilot Instructions
        run: |
          python -c @"
          import pathlib, sys
          instructions = pathlib.Path('.github/copilot-instructions.md')
          if not instructions.exists():
              print('FAIL: copilot-instructions.md not found')
              sys.exit(1)
          content = instructions.read_text(encoding='utf-8')
          required = ['MANDATORY PROTOCOL', 'Agent Routing', 'Security Rules', 'Terminal Rules']
          for r in required:
              assert r in content, f'Missing section: {r}'
              print(f'  OK: {r}')
          print(f'Copilot instructions: {len(content)} chars, all sections present')
          "@
      - name: Validate Agent Definition
        run: |
          python -c @"
          import pathlib, sys
          agents_md = pathlib.Path('AGENTS.md')
          if not agents_md.exists():
              print('FAIL: AGENTS.md not found')
              sys.exit(1)
          content = agents_md.read_text(encoding='utf-8')
          required = ['MANDATORY PROTOCOL', 'Agent Routing', 'Bridge Architecture']
          for r in required:
              assert r in content, f'Missing section in AGENTS.md: {r}'
              print(f'  OK: {r}')
          print('AGENTS.md validation: PASSED')
          "@
      - name: Validate Skills
        run: |
          python -c @"
          import pathlib, json
          skills_dir = pathlib.Path('skills')
          if skills_dir.exists():
              skills = [p for p in skills_dir.iterdir() if p.is_dir()]
              print(f'Skills found: {len(skills)}')
              for s in skills:
                  skill_file = s / 'skill.json'
                  if skill_file.exists():
                      data = json.loads(skill_file.read_text(encoding='utf-8'))
                      print(f'  OK: {s.name} - {data.get(\"name\", \"unnamed\")}')
                  else:
                      print(f'  WARN: {s.name} has no skill.json')
          else:
              print('No skills directory found')
          "@
      - name: Validate @slate Extension
        run: |
          $extensionPath = "plugins/slate-copilot/package.json"
          if (Test-Path $extensionPath) {
            $pkg = Get-Content $extensionPath -Raw | ConvertFrom-Json
            Write-Host "Extension: $($pkg.displayName) v$($pkg.version)"

            # Verify chat participant exists
            $participants = $pkg.contributes.chatParticipants
            if ($participants) {
              Write-Host "Chat Participant: $($participants[0].id)"
              Write-Host "Full Name: $($participants[0].fullName)"
            }

            # Verify LM tools
            $tools = $pkg.contributes.languageModelTools
            if ($tools) {
              Write-Host "LM Tools: $($tools.Count)"
              foreach ($t in $tools) { Write-Host "  $($t.name): $($t.displayName)" }
            }
          } else {
            Write-Host "WARN: @slate extension not found"
          }
      - name: Validate Copilot Agent Bridge
        run: |
          python -c @"
          import importlib
          # Verify bridge module exists
          bridge = importlib.import_module('slate.copilot_agent_bridge')
          print(f'Copilot Agent Bridge: OK')

          # Verify copilot runner exists
          runner = importlib.import_module('slate.copilot_slate_runner')
          print(f'Copilot Slate Runner: OK')

          print('Copilot SDK integration: PASSED')
          "@

  # ------------ Transformer Pipeline Validation (Matrix across analysis types) ------------
  transformer-analysis:
    name: Transformer (${{ matrix.analysis }})
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 15
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    strategy:
      fail-fast: false
      matrix:
        analysis:
          - status
          - security-scan
          - commit-classify
        include:
          - analysis: status
            script: 'python slate/slate_transformers.py --status'
          - analysis: security-scan
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json;
              p = SlateTransformerPipeline();
              r = p.security_scan('import os; os.system(input())');
              print(json.dumps(r, indent=2))
              "
          - analysis: commit-classify
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json, subprocess;
              log = subprocess.run(['git', 'log', '--max-count=5', '--pretty=format:%s'],
                capture_output=True, text=True).stdout.strip().split('\n');
              p = SlateTransformerPipeline();
              r = p.classify_commits(log);
              print(json.dumps(r, indent=2))
              "
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Run ${{ matrix.analysis }}
        run: ${{ matrix.script }}
        continue-on-error: true

  summary:
    name: CI Summary
    runs-on: [self-hosted, slate]
    needs: [lint, unit-tests, sdk-validation, security, slate-checks, gpu-validation, ai-code-review, ai-inference, copilot-sdk, semantic-kernel, transformer-analysis]
    if: always()
    steps:
      - name: Generate summary
        run: |
          "## SLATE CI Summary" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Job | Status |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "|-----|--------|" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Lint | ${{ needs.lint.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Unit Tests (matrix) | ${{ needs.unit-tests.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SDK Validation | ${{ needs.sdk-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Security | ${{ needs.security.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SLATE Checks | ${{ needs.slate-checks.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| GPU Validation | ${{ needs.gpu-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| AI Code Review | ${{ needs.ai-code-review.result || 'skipped' }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| AI Inference | ${{ needs.ai-inference.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Copilot SDK | ${{ needs.copilot-sdk.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Semantic Kernel | ${{ needs.semantic-kernel.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Transformer Analysis (matrix) | ${{ needs.transformer-analysis.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
