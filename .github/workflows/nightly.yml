# Modified: 2026-02-08T15:00:00Z | Author: COPILOT | Change: Integrate with stable release validation, add release health check
# Modified: 2026-02-07T18:30:00Z | Author: COPILOT | Change: Add matrix strategies for test suites and transformer analysis
#
# SLATE Nightly â€” Full Test Suite + Release Health
# Runs at 04:00 UTC daily. Feeds into stable-release.yml validation (03:30 UTC).
# Reference: https://github.com/features/actions
name: SLATE Nightly

on:
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:

concurrency:
  group: nightly-${{ github.ref }}
  cancel-in-progress: false  # Let nightly runs complete

permissions:
  contents: read

defaults:
  run:
    shell: powershell

jobs:
  full-test-suite:
    name: Tests (${{ matrix.suite }})
    runs-on: [self-hosted, slate]
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        suite:
          - core
          - security
          - agents
          - ml
        include:
          - suite: core
            pattern: 'test_slate_runtime or test_slate_workflow or test_feature_flags or test_install_tracker or test_slate_benchmark or test_watcher or test_module_registry or test_slate_terminal_monitor or test_runner_cost_tracker or test_runner_fallback'
          - suite: security
            pattern: 'test_pii_scanner or test_action_guard or test_sdk_source_guard or test_slate_workflow_analyzer'
          - suite: agents
            pattern: 'test_agent_registry or test_agent_plugins or test_slate_runner_manager or test_slate_runner_benchmark or test_slate_real_multi_runner or test_copilot_slate_runner or test_slate_discussion_manager or test_slate_project_board'
          - suite: ml
            pattern: 'test_ml_orchestrator or test_slate_model_trainer or test_slate_gpu_manager or test_slate_warmup or test_slate_unified_autonomous or test_integrated_autonomous_loop or test_slate_hardware_optimizer'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Install test deps
        run: pip install pytest pytest-cov pytest-asyncio --quiet 2>$null
        continue-on-error: true
      - name: Run ${{ matrix.suite }} tests with coverage
        run: python -m pytest tests/ -v -k "${{ matrix.pattern }}" --tb=long --cov=slate --cov-report=term-missing
        continue-on-error: true

  slate-health:
    name: SLATE Health
    runs-on: [self-hosted, slate]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: SLATE status
        run: python slate/slate_status.py --quick
      - name: K8s health check
        run: python slate/slate_k8s_deploy.py --health
        continue-on-error: true
      - name: Validate task queue
        run: |
          python -c @"
          import json, pathlib
          p = pathlib.Path('current_tasks.json')
          if p.exists():
              data = json.loads(p.read_text())
              tasks = data.get('tasks', data) if isinstance(data, dict) else data
              print(f'Task queue: {len(tasks)} tasks')
              for t in tasks[:5]:
                  status = t.get('status', 'unknown')
                  title = t.get('title', t.get('name', 'unnamed'))
                  print(f'  [{status}] {title}')
          else:
              print('No task queue found')
          "@

  sdk-import-audit:
    name: SDK Import Audit
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Audit all modules
        run: |
          python -c @"
          import importlib, pathlib, sys
          ok, fail = [], []
          for f in sorted(pathlib.Path('slate').glob('*.py')):
              mod = f.stem
              if mod.startswith('_'): continue
              try:
                  importlib.import_module(f'slate.{mod}')
                  ok.append(mod)
                  print(f'  OK: slate.{mod}')
              except Exception as e:
                  fail.append((mod, str(e)))
                  print(f'  FAIL: slate.{mod}: {e}')
          print(f'Audit: {len(ok)} OK, {len(fail)} failed')
          if fail: sys.exit(1)
          "@

  dependency-check:
    name: Dependency Check
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Check outdated packages
        run: pip list --outdated 2>$null | Select-Object -First 20
        continue-on-error: true
      - name: Validate requirements
        run: |
          python -c @"
          import pathlib
          req = pathlib.Path('requirements.txt')
          if req.exists():
              lines = [l.strip() for l in req.read_text().splitlines() if l.strip() and not l.startswith('#')]
              print(f'Requirements: {len(lines)} packages')
              for l in lines[:10]:
                  print(f'  {l}')
          "@

  ai-codebase-analysis:
    name: AI Codebase Analysis
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 60
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Check Ollama
        id: ollama
        run: |
          $available = $false
          try {
            $response = Invoke-RestMethod -Uri "http://127.0.0.1:11434/api/tags" -TimeoutSec 5
            if ($response) { $available = $true }
          } catch { }
          "available=$available" | Out-File -Append $env:GITHUB_OUTPUT
      - name: Warmup AI Models
        if: steps.ollama.outputs.available == 'True'
        run: |
          python slate/slate_ai_orchestrator.py --warmup
      - name: Full Codebase Analysis
        if: steps.ollama.outputs.available == 'True'
        run: |
          Write-Host "Running AI-powered codebase analysis..."
          Write-Host ""
          python slate/slate_ai_orchestrator.py --analyze-codebase --json > codebase_analysis.json

          $analysis = Get-Content codebase_analysis.json -Raw | ConvertFrom-Json
          Write-Host "Files analyzed: $($analysis.total_files)"
          Write-Host "Code quality: $($analysis.code_quality)"

          "## AI Codebase Analysis" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "**Files Analyzed**: $($analysis.total_files)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "**Code Quality**: $($analysis.code_quality)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "**Recommendations**: $($analysis.recommendations_count)" | Out-File -Append $env:GITHUB_STEP_SUMMARY
      - name: Update Documentation (if needed)
        if: steps.ollama.outputs.available == 'True'
        run: |
          python slate/slate_ai_orchestrator.py --update-docs
        continue-on-error: true
      - name: Upload Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-codebase-analysis
          path: codebase_analysis.json
          retention-days: 30
          if-no-files-found: ignore

  # Modified: 2026-02-08T22:00:00Z | Author: COPILOT | Change: Add Semantic Kernel nightly validation
  # ------------ Semantic Kernel Nightly Validation ------------
  semantic-kernel-nightly:
    name: SK Nightly Validation
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 20
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: SK Version & Dependency Check
        run: |
          python -c @"
          import semantic_kernel
          import openai
          print(f'semantic-kernel: {semantic_kernel.__version__}')
          print(f'openai: {openai.__version__}')
          
          # Verify version constraints from requirements.txt
          sk_major = int(semantic_kernel.__version__.split('.')[0])
          assert sk_major >= 1, f'SK version {semantic_kernel.__version__} too old (need >= 1.x)'
          assert sk_major < 2, f'SK version {semantic_kernel.__version__} too new (need < 2.x)'
          print('Version constraints: PASSED')
          "@
      - name: SK Full Status
        run: python slate/slate_semantic_kernel.py --status
      - name: SK Plugins Audit
        run: |
          python -c @"
          from semantic_kernel import Kernel
          from slate.slate_semantic_kernel import _register_slate_plugins
          kernel = Kernel()
          _register_slate_plugins(kernel)
          
          expected_plugins = {'slate_system', 'slate_search', 'slate_agents'}
          actual_plugins = set(kernel.plugins.keys())
          missing = expected_plugins - actual_plugins
          extra = actual_plugins - expected_plugins
          
          print(f'Expected plugins: {expected_plugins}')
          print(f'Actual plugins: {actual_plugins}')
          
          if missing:
              print(f'MISSING plugins: {missing}')
              exit(1)
          if extra:
              print(f'Extra plugins: {extra} (OK)')
          
          # Verify expected functions in slate_system
          system_funcs = list(kernel.plugins['slate_system'].keys())
          expected_funcs = ['get_system_status', 'get_runtime_integrations', 'get_workflow_status', 'get_gpu_status']
          for f in expected_funcs:
              assert f in system_funcs, f'Missing function: {f}'
              print(f'  OK: slate_system.{f}')
          
          print(f'Plugin audit: PASSED ({sum(len(p) for p in kernel.plugins.values())} total functions)')
          "@
      - name: SK Benchmark (All Roles)
        run: python slate/slate_semantic_kernel.py --benchmark
        continue-on-error: true
      - name: SK ActionGuard Enforcement Test
        run: |
          python -c @"
          import asyncio
          from slate.slate_semantic_kernel import invoke_sk
          async def test_guard():
              # Test that ActionGuard blocks dangerous prompts
              result = await invoke_sk(
                  prompt='safe prompt about SLATE architecture',
                  model_role='fast',
                  enable_memory=False,
                  enable_plugins=False,
              )
              assert '[ActionGuard BLOCKED]' not in result, 'Safe prompt was blocked'
              print(f'Safe prompt: PASSED ({len(result)} chars)')
          asyncio.run(test_guard())
          "@
        continue-on-error: true
      - name: SK Model Coverage Check
        run: |
          python -c @"
          from slate.slate_semantic_kernel import SK_MODEL_MAP, SK_FALLBACK_MAP, _get_available_models
          available = _get_available_models()
          print(f'Available Ollama models: {len(available)}')
          
          coverage = {}
          for role, model in SK_MODEL_MAP.items():
              if model in available:
                  coverage[role] = ('primary', model)
              elif SK_FALLBACK_MAP.get(model) in available:
                  coverage[role] = ('fallback', SK_FALLBACK_MAP[model])
              else:
                  coverage[role] = ('missing', model)
          
          for role, (status, model) in coverage.items():
              icon = 'OK' if status == 'primary' else ('FALLBACK' if status == 'fallback' else 'MISSING')
              print(f'  [{icon}] {role:12s} -> {model}')
          
          missing_count = sum(1 for s, _ in coverage.values() if s == 'missing')
          if missing_count > 0:
              print(f'WARNING: {missing_count} roles without available models')
          else:
              print(f'Model coverage: COMPLETE ({len(coverage)} roles)')
          "@

  # ------------ Nightly Transformer Analysis (Matrix) ------------
  transformer-nightly:
    name: Transformer (${{ matrix.task }})
    runs-on: [self-hosted, slate, gpu, gpu-2]
    timeout-minutes: 20
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    strategy:
      fail-fast: false
      matrix:
        task:
          - security-scan
          - benchmark
          - commit-history
        include:
          - task: security-scan
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json, pathlib;
              p = SlateTransformerPipeline();
              results = [];
              for f in sorted(pathlib.Path('slate').glob('*.py'))[:10]:
                code = f.read_text(encoding='utf-8')[:2000];
                r = p.security_scan(code);
                results.append({'file': str(f), 'scan': r});
              print(json.dumps({'scanned': len(results), 'results': results}, indent=2))
              "
          - task: benchmark
            script: 'python slate/slate_transformers.py --benchmark --json'
          - task: commit-history
            script: >-
              python -c "
              from slate.slate_transformers import SlateTransformerPipeline;
              import json, subprocess;
              log = subprocess.run(['git', 'log', '--max-count=20', '--pretty=format:%s'],
                capture_output=True, text=True).stdout.strip().split('\n');
              p = SlateTransformerPipeline();
              r = p.classify_commits(log);
              print(json.dumps(r, indent=2))
              "
    steps:
      - uses: actions/checkout@v6
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Run ${{ matrix.task }}
        run: ${{ matrix.script }}
        continue-on-error: true

  summary:
    name: Nightly Summary
    runs-on: [self-hosted, slate]
    needs: [full-test-suite, slate-health, sdk-import-audit, dependency-check, ai-codebase-analysis, transformer-nightly, semantic-kernel-nightly]
    if: always()
    steps:
      - name: Generate summary
        run: |
          "## Nightly Health Summary" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Check | Status |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "|-------|--------|" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Full Tests (matrix) | ${{ needs.full-test-suite.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SLATE Health | ${{ needs.slate-health.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SDK Import Audit | ${{ needs.sdk-import-audit.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Dependency Check | ${{ needs.dependency-check.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| AI Analysis | ${{ needs.ai-codebase-analysis.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Transformer Analysis (matrix) | ${{ needs.transformer-nightly.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Semantic Kernel | ${{ needs.semantic-kernel-nightly.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "Run at $(Get-Date -Format 'yyyy-MM-dd HH:mm UTC')" | Out-File -Append $env:GITHUB_STEP_SUMMARY

  # Modified: 2026-02-08T15:00:00Z | Author: COPILOT | Change: Add release health check to nightly
  release-health:
    name: Release Health Check
    runs-on: [self-hosted, slate]
    needs: [full-test-suite, slate-health]
    if: always()
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: recursive
      - name: Setup Python
        run: |
          "$env:GITHUB_WORKSPACE\\.venv\\Scripts" | Out-File -Append $env:GITHUB_PATH
      - name: Validate Stable Release
        run: |
          python slate/slate_release_manager.py --validate
        continue-on-error: true
      - name: Check for Updates
        run: |
          python slate/slate_release_manager.py --check-update
        continue-on-error: true
      - name: Live Update Pre-flight
        run: |
          python slate/slate_live_update.py --check
        continue-on-error: true
      - name: SDK Bridge Check
        run: |
          python slate/slate_copilot_sdk_bridge.py --check
        continue-on-error: true
