#!/usr/bin/env python3
"""
SLATE Unified Autonomous Loop
==============================
# Modified: 2026-02-07T04:30:00Z | Author: COPILOT | Change: Initial implementation

Adaptive autonomous agent that discovers, routes, and executes tasks
using local GPU inference. Pulls from KANBAN, tech tree, and codebase
analysis to find work, then uses Ollama for intelligent task processing.

Architecture:
    KANBAN Board --> Task Discovery --> ML Classification --> Agent Routing
         ^              |                    |                    |
         |              v                    v                    v
    Project Board   Codebase Scan     Ollama Inference    Runner Dispatch
         |              |                    |                    |
         v              v                    v                    v
    Completion <--- Result Tracking <--- Execution <--------- GPU Workers

Usage:
    python slate/slate_unified_autonomous.py --run --max 50    # Run loop
    python slate/slate_unified_autonomous.py --status          # Show status
    python slate/slate_unified_autonomous.py --discover        # Discover tasks
    python slate/slate_unified_autonomous.py --single          # Run one task
"""

import argparse
import json
import re
import subprocess
import sys
import time
from datetime import datetime, timezone
from pathlib import Path

# Modified: 2026-02-10T08:00:00Z | Author: COPILOT | Change: Add _NO_WINDOW to suppress console popups on Windows
_NO_WINDOW = subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0

# Modified: 2026-02-07T04:30:00Z | Author: COPILOT | Change: workspace setup
WORKSPACE_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(WORKSPACE_ROOT))

# Force UTF-8
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

TASK_FILE = WORKSPACE_ROOT / "current_tasks.json"
STATE_FILE = WORKSPACE_ROOT / ".slate_autonomous_state.json"
LOG_DIR = WORKSPACE_ROOT / "slate_logs" / "autonomous"

# Modified: 2026-02-07T12:00:00Z | Author: COPILOT | Change: Add COPILOT_CHAT agent routing for @slate participant bridge
# Agent routing patterns
AGENT_PATTERNS = {
    "ALPHA": ["implement", "code", "build", "fix", "create", "add", "refactor", "write"],
    "BETA": ["test", "validate", "verify", "coverage", "check", "lint", "format"],
    "GAMMA": ["analyze", "plan", "research", "document", "review", "design"],
    "DELTA": ["claude", "mcp", "sdk", "integration", "api", "plugin"],
    "EPSILON": ["spec", "specification", "architecture", "blueprint", "schema", "rfc", "capacity"],
    "ZETA": ["benchmark", "performance", "profile", "throughput", "latency", "optimize", "capacity"],
    "COPILOT": ["complex", "multi-step", "orchestrate", "deploy", "release", "kubernetes", "k8s", "pod", "cluster"],
    "COPILOT_CHAT": ["diagnose", "investigate", "troubleshoot", "interactive", "explain", "full protocol", "comprehensive"],
}


class UnifiedAutonomousLoop:
    """Adaptive autonomous agent loop for SLATE."""

    # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Add session-level completed_ids to prevent ephemeral task re-execution
    def __init__(self):
        self.workspace = WORKSPACE_ROOT
        self.state = self._load_state()
        self.ml = None  # Lazy-loaded ML orchestrator
        self._slate_models_checked = False
        self._github_models = None  # Lazy-loaded GitHub Models client
        self.completed_ids: set[str] = set()  # Session-level tracking for ephemeral tasks
        LOG_DIR.mkdir(parents=True, exist_ok=True)

    def _get_ml(self):
        """Lazy-load ML orchestrator and ensure SLATE models exist."""
        if self.ml is None:
            from slate.ml_orchestrator import MLOrchestrator
            self.ml = MLOrchestrator()
            # Build SLATE models on first access if needed
            if not self._slate_models_checked:
                self._slate_models_checked = True
                try:
                    result = self.ml.ensure_slate_models()
                    if result.get("status") == "built":
                        self._log(f"Built {result['built']} SLATE models")
                    elif result.get("status") == "all_built":
                        self._log("All SLATE models ready")
                except Exception as e:
                    self._log(f"SLATE model check failed: {e}", "WARN")
        return self.ml

    # Modified: 2026-02-09T02:00:00Z | Author: COPILOT | Change: Add GitHub Models cloud fallback for autonomous inference
    def _get_github_models(self):
        """Lazy-load GitHub Models client for cloud inference fallback."""
        if self._github_models is None:
            try:
                from slate.slate_github_models import GitHubModelsWithFallback
                client = GitHubModelsWithFallback()
                if client.github_client.authenticated:
                    self._github_models = client
                    self._log("GitHub Models client initialized for cloud fallback")
                else:
                    self._github_models = False  # Mark as unavailable
            except Exception:
                self._github_models = False
        return self._github_models if self._github_models else None

    def _infer_with_fallback(self, prompt: str, task_type: str = "general",
                             system: str = "", max_tokens: int = 1024,
                             temperature: float = 0.5) -> dict:
        """Run inference with Ollama -> GitHub Models fallback chain.

        Returns dict with keys: response, model, tokens, tok_per_sec, source.
        """
        # Try Ollama first (fastest, local, no rate limits)
        try:
            ml = self._get_ml()
            if ml.ollama.is_running():
                result = ml.infer(prompt, task_type=task_type, system=system,
                                  max_tokens=max_tokens, temperature=temperature)
                if result.get("response"):
                    result["source"] = "ollama"
                    return result
        except Exception as e:
            self._log(f"  Ollama inference failed: {e}", "WARN")

        # Fallback to GitHub Models (free cloud)
        gh = self._get_github_models()
        if gh:
            try:
                # Map task_type to SLATE role
                role_map = {
                    "code_generation": "code", "code_review": "code",
                    "planning": "planner", "analysis": "analysis",
                    "general": "general",
                }
                role = role_map.get(task_type, "general")
                resp = gh.chat(prompt, role=role, system=system,
                               max_tokens=max_tokens, temperature=temperature)
                return {
                    "response": resp.content,
                    "model": resp.model,
                    "tokens": resp.tokens,
                    "tok_per_sec": 0,
                    "source": "github_models",
                }
            except Exception as e:
                self._log(f"  GitHub Models fallback failed: {e}", "WARN")

        return {"response": "", "model": "none", "tokens": 0, "tok_per_sec": 0,
                "source": "none", "error": "All inference backends unavailable"}

    def _load_state(self) -> dict:
        """Load autonomous loop state."""
        if STATE_FILE.exists():
            return json.loads(STATE_FILE.read_text(encoding="utf-8"))
        return {
            "started_at": None,
            "tasks_discovered": 0,
            "tasks_completed": 0,
            "tasks_failed": 0,
            "cycles": 0,
            "last_cycle": None,
            "adaptations": [],
            "history": [],
        }

    def _save_state(self):
        """Save autonomous loop state."""
        STATE_FILE.write_text(json.dumps(self.state, indent=2, default=str), encoding="utf-8")

    def _log(self, msg: str, level: str = "INFO"):
        """Log a message."""
        ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
        line = f"[{ts}] [{level}] {msg}"
        print(line)
        log_file = LOG_DIR / f"autonomous_{datetime.now().strftime('%Y%m%d')}.log"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(line + "\n")

    # ------------------------------------------------------------------
    # Task Discovery
    # ------------------------------------------------------------------

    def discover_tasks(self) -> list[dict]:
        """Discover pending tasks from all sources."""
        tasks = []

        # Source 1: current_tasks.json (pending tasks)
        tasks.extend(self._discover_from_task_file())

        # Source 2: KANBAN board
        tasks.extend(self._discover_from_kanban())

        # Source 3: GitHub Issues (labeled for autonomy)
        # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: GitHub Issues integration
        tasks.extend(self._discover_from_github_issues())

        # Source 4: Codebase analysis (TODOs, FIXMEs, errors)
        tasks.extend(self._discover_from_codebase())

        # Source 5: Test coverage gaps
        tasks.extend(self._discover_from_coverage())

        # Source 6: Kubernetes pod health issues
        # Modified: 2026-02-09T04:00:00Z | Author: COPILOT | Change: Add K8s health discovery
        tasks.extend(self._discover_from_kubernetes())

        # Deduplicate by title similarity
        seen_titles = set()
        unique = []
        for task in tasks:
            key = task.get("title", "")[:50].lower()
            task_id = task.get("id", "")
            # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Filter out session-completed ephemeral tasks
            if key in seen_titles:
                continue
            if task_id and task_id in self.completed_ids:
                continue
            seen_titles.add(key)
            unique.append(task)

        # Modified: 2026-02-08T21:45:00Z | Author: COPILOT | Change: Sort discovered tasks by priority so critical/high always surface first
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        unique.sort(key=lambda t: priority_order.get(t.get("priority", "medium"), 2))

        self.state["tasks_discovered"] += len(unique)
        self._save_state()

        return unique

    # Modified: 2026-02-10T12:15:00Z | Author: COPILOT | Change: Reset stale tasks to pending instead of silently dropping them, preventing short-circuit on stale tasks
    def _discover_from_task_file(self) -> list[dict]:
        """Load pending tasks from current_tasks.json.
        
        Stale tasks (in_progress > 4h) are automatically reset to 'pending'
        so they can be re-attempted rather than silently filtered out.
        """
        if not TASK_FILE.exists():
            return []
        try:
            data = json.loads(TASK_FILE.read_text(encoding="utf-8"))
            all_tasks = data.get("tasks", [])
            
            # Reset stale tasks back to pending
            stale_reset = False
            for t in all_tasks:
                if t.get("status") == "in_progress" and self._is_stale(t):
                    self._log(f"Resetting stale task: {t.get('title', t.get('id', 'unknown'))}", "WARN")
                    t["status"] = "pending"
                    t["started_at"] = None
                    t["stale_reset_count"] = t.get("stale_reset_count", 0) + 1
                    stale_reset = True
            
            # Persist the reset
            if stale_reset:
                data["tasks"] = all_tasks
                TASK_FILE.write_text(json.dumps(data, indent=2), encoding="utf-8")
            
            pending = [
                t for t in all_tasks
                if t.get("status") in ("pending", "in_progress")
                and t.get("stale_reset_count", 0) < 3  # Max 3 resets before giving up
            ]
            return pending
        except Exception as e:
            self._log(f"Error reading task file: {e}", "WARN")
            return []

    # Modified: 2026-02-08T06:00:00Z | Author: COPILOT | Change: Fix KANBAN discovery to return only newly synced items, not duplicate task file
    def _discover_from_kanban(self) -> list[dict]:
        """Sync pending items from KANBAN board and return only new items.

        Previous implementation re-called _discover_from_task_file() after sync,
        which duplicated Source 1 results and missed items already in the file
        with completed/stale status.  Now we snapshot task IDs before sync,
        re-read after sync, and return only genuinely new pending items.
        """
        # Snapshot existing task IDs before sync
        existing_ids: set[str] = set()
        if TASK_FILE.exists():
            try:
                data = json.loads(TASK_FILE.read_text(encoding="utf-8"))
                for t in data.get("tasks", []):
                    existing_ids.add(t.get("id", ""))
            except Exception:
                pass

        try:
            result = subprocess.run(
                [sys.executable, str(self.workspace / "slate" / "slate_project_board.py"), "--sync"],
                capture_output=True, text=True, timeout=30, cwd=str(self.workspace),
                encoding="utf-8", errors="replace",
                creationflags=_NO_WINDOW,
            )
            if result.returncode != 0:
                return []
        except Exception as e:
            self._log(f"KANBAN sync failed: {e}", "WARN")
            return []

        # Re-read and return ONLY items that were added by the sync
        if not TASK_FILE.exists():
            return []
        try:
            data = json.loads(TASK_FILE.read_text(encoding="utf-8"))
            new_tasks = [
                t for t in data.get("tasks", [])
                if t.get("id", "") not in existing_ids
                and t.get("status") in ("pending", "in_progress")
                and not self._is_stale(t)
            ]
            return new_tasks
        except Exception as e:
            self._log(f"Error reading task file after KANBAN sync: {e}", "WARN")
            return []

    # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: GitHub Issues task discovery
    def _discover_from_github_issues(self) -> list[dict]:
        """Pull open GitHub Issues labeled for autonomous processing."""
        tasks = []
        try:
            # Get GitHub token from git credential manager
            cred_result = subprocess.run(
                ["git", "credential", "fill"],
                input="protocol=https\nhost=github.com\n",
                capture_output=True, text=True, timeout=10,
                creationflags=_NO_WINDOW,
            )
            token = None
            for line in cred_result.stdout.splitlines():
                if line.startswith("password="):
                    token = line.split("=", 1)[1]
                    break
            if not token:
                return []

            # Fetch issues with 'autonomous' or 'slate-task' labels
            import urllib.request
            import urllib.error
            api_url = (
                "https://api.github.com/repos/SynchronizedLivingArchitecture/S.L.A.T.E"
                "/issues?state=open&labels=autonomous&per_page=10"
            )
            req = urllib.request.Request(api_url, headers={
                "Authorization": f"token {token}",
                "Accept": "application/vnd.github.v3+json",
            })
            with urllib.request.urlopen(req, timeout=15) as resp:
                issues = json.loads(resp.read().decode("utf-8"))

            for issue in issues:
                if issue.get("pull_request"):
                    continue  # Skip PRs
                labels = [l.get("name", "") for l in issue.get("labels", [])]
                priority = "high" if "priority:high" in labels else (
                    "critical" if "priority:critical" in labels else "medium"
                )
                tasks.append({
                    "id": f"gh_issue_{issue['number']}",
                    "title": issue.get("title", ""),
                    "description": (issue.get("body", "") or "")[:500],
                    "priority": priority,
                    "source": "github_issues",
                    "issue_number": issue["number"],
                    "issue_url": issue.get("html_url", ""),
                    "status": "pending",
                })

        except Exception as e:
            self._log(f"GitHub Issues fetch failed: {e}", "WARN")
        return tasks[:5]  # Cap at 5

    # Modified: 2026-02-07T12:50:00Z | Author: COPILOT | Change: Fix self-scan false positives — exclude scanner's own pattern definitions
    def _discover_from_codebase(self) -> list[dict]:
        """Find TODOs and FIXMEs in codebase."""
        tasks = []
        # Split pattern markers from colon to prevent self-detection
        _markers = [("TO" + "DO:"), ("FIX" + "ME:"), ("HA" + "CK:"), ("BU" + "G:")]
        scan_dirs = ["slate", "agents"]
        # Exclude this file to avoid detecting pattern definitions as issues
        self_path = Path(__file__).resolve()

        for dir_name in scan_dirs:
            dir_path = self.workspace / dir_name
            if not dir_path.exists():
                continue
            for py_file in dir_path.rglob("*.py"):
                # Skip self to prevent false positives from pattern string literals
                if py_file.resolve() == self_path:
                    continue
                try:
                    content = py_file.read_text(encoding="utf-8", errors="replace")
                    for i, line in enumerate(content.split("\n"), 1):
                        stripped = line.strip()
                        # Only match patterns in comments, not string literals
                        if not stripped.startswith("#"):
                            continue
                        for pattern in _markers:
                            if pattern in stripped:
                                rel = str(py_file.relative_to(self.workspace))
                                msg = line.strip().lstrip("#").strip()
                                tasks.append({
                                    "id": f"codebase_{rel}_{i}",
                                    "title": f"{pattern} in {rel}:{i}",
                                    "description": msg,
                                    "priority": "low" if pattern == "TODO:" else "medium",
                                    "source": "codebase_scan",
                                    "file_paths": rel,
                                    "status": "pending",
                                })
                except Exception:
                    pass
        return tasks[:10]  # Cap at 10 to avoid noise

    # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Fix coverage discovery to check test content and avoid re-execution loop
    def _discover_from_coverage(self) -> list[dict]:
        """Find files without tests."""
        tasks = []
        test_dir = self.workspace / "tests"
        slate_dir = self.workspace / "slate"

        if not slate_dir.exists():
            return []

        tested_modules = set()
        if test_dir.exists():
            for test_file in test_dir.rglob("test_*.py"):
                # Extract module name from test filename
                name = test_file.stem.replace("test_", "")
                tested_modules.add(name)

        for py_file in slate_dir.glob("*.py"):
            if py_file.name.startswith("_"):
                continue
            module = py_file.stem
            if module in tested_modules:
                continue
            # Skip if already completed in this session
            task_id = f"coverage_{module}"
            if task_id in self.completed_ids:
                continue
            tasks.append({
                "id": task_id,
                "title": f"Add tests for slate/{module}.py",
                "description": f"No test file found for slate/{module}.py. Create tests/test_{module}.py with unit tests.",
                "priority": "low",
                "source": "coverage_gap",
                "file_paths": f"slate/{module}.py",
                "status": "pending",
                "test_target": f"tests/test_{module}.py",
            })
        return tasks[:5]

    # Modified: 2026-02-09T04:00:00Z | Author: COPILOT | Change: Add K8s pod health discovery source
    def _discover_from_kubernetes(self) -> list[dict]:
        """Discover unhealthy K8s pods and failed CronJobs in the SLATE namespace."""
        tasks = []
        try:
            # Check for non-Running pods
            result = subprocess.run(
                ["kubectl", "get", "pods", "-n", "slate",
                 "-o", "jsonpath={range .items[*]}{.metadata.name}|{.status.phase}|{.status.containerStatuses[0].restartCount}\n{end}"],
                capture_output=True, text=True, timeout=10,
                encoding="utf-8", errors="replace",
                creationflags=_NO_WINDOW,
            )
            if result.returncode == 0:
                for line in result.stdout.strip().split("\n"):
                    if not line.strip():
                        continue
                    parts = line.strip().split("|")
                    if len(parts) >= 2:
                        name, phase = parts[0], parts[1]
                        restarts = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0
                        if phase not in ("Running", "Succeeded"):
                            task_id = f"k8s_pod_{name}"
                            # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Skip K8s tasks already completed this session
                            if task_id in self.completed_ids:
                                continue
                            tasks.append({
                                "id": task_id,
                                "title": f"Fix K8s pod {name} (status: {phase})",
                                "description": f"Pod {name} in slate namespace is {phase} with {restarts} restarts",
                                "priority": "high",
                                "source": "kubernetes",
                                "status": "pending",
                            })
                        elif restarts > 5:
                            task_id = f"k8s_restart_{name}"
                            if task_id in self.completed_ids:
                                continue
                            tasks.append({
                                "id": task_id,
                                "title": f"Investigate K8s pod {name} ({restarts} restarts)",
                                "description": f"Pod {name} is Running but has {restarts} container restarts",
                                "priority": "medium",
                                "source": "kubernetes",
                                "status": "pending",
                            })

            # Check for failed CronJobs
            cj_result = subprocess.run(
                ["kubectl", "get", "jobs", "-n", "slate",
                 "-o", "jsonpath={range .items[*]}{.metadata.name}|{.status.succeeded}|{.status.failed}\n{end}"],
                capture_output=True, text=True, timeout=10,
                encoding="utf-8", errors="replace",
                creationflags=_NO_WINDOW,
            )
            if cj_result.returncode == 0:
                for line in cj_result.stdout.strip().split("\n"):
                    if not line.strip():
                        continue
                    parts = line.strip().split("|")
                    if len(parts) >= 3:
                        name = parts[0]
                        failed = int(parts[2]) if parts[2].isdigit() else 0
                        if failed > 0:
                            task_id = f"k8s_job_{name}"
                            if task_id in self.completed_ids:
                                continue
                            tasks.append({
                                "id": task_id,
                                "title": f"Fix failed K8s job {name}",
                                "description": f"Job {name} has {failed} failure(s) in slate namespace",
                                "priority": "medium",
                                "source": "kubernetes",
                                "status": "pending",
                            })
        except FileNotFoundError:
            pass  # kubectl not available
        except Exception as e:
            self._log(f"K8s discovery error: {e}", "WARN")
        return tasks[:5]

    def _is_stale(self, task: dict) -> bool:
        """Check if a task is stale (in-progress too long)."""
        started = task.get("started_at")
        if not started or task.get("status") != "in_progress":
            return False
        try:
            start_dt = datetime.fromisoformat(started.replace("Z", "+00:00"))
            elapsed = (datetime.now(timezone.utc) - start_dt).total_seconds()
            return elapsed > 4 * 3600  # 4 hours
        except Exception:
            return False

    # ------------------------------------------------------------------
    # Task Classification & Routing
    # ------------------------------------------------------------------

    def classify_task(self, task: dict) -> dict:
        """Classify task and determine routing using ML."""
        title = task.get("title", "")
        desc = task.get("description", "")
        combined = f"{title}. {desc}"

        # Fast pattern matching first
        for agent, patterns in AGENT_PATTERNS.items():
            for pattern in patterns:
                if pattern in combined.lower():
                    return {"agent": agent, "method": "pattern", "confidence": 0.9}

        # Modified: 2026-02-08T10:00:00Z | Author: COPILOT | Change: Add Semantic Kernel as enhanced classification backend
        # Try Semantic Kernel agent routing (higher quality with function-calling)
        try:
            from slate.slate_semantic_kernel import get_sk_status
            sk_info = get_sk_status()
            if sk_info.get("semantic_kernel", {}).get("available"):
                import asyncio
                from slate.slate_semantic_kernel import create_slate_kernel
                kernel = asyncio.get_event_loop().run_until_complete(create_slate_kernel())
                if kernel:
                    # Use SK's SlateAgentPlugin.route_task
                    plugins = kernel.plugins
                    if "SlateAgent" in plugins:
                        route_fn = plugins["SlateAgent"]["route_task"]
                        result_text = route_fn.method(task_description=combined)
                        # Parse the agent name from SK response
                        for agent_name in AGENT_PATTERNS.keys():
                            if agent_name in str(result_text).upper():
                                return {
                                    "agent": agent_name,
                                    "method": "semantic_kernel",
                                    "confidence": 0.85,
                                    "sk_response": str(result_text)[:200],
                                }
        except Exception:
            pass

        # Fall back to ML inference if Ollama available
        try:
            ml = self._get_ml()
            if ml.ollama.is_running():
                result = ml.classify_task(combined)
                return {
                    "agent": result.get("routed_agent", "ALPHA"),
                    "method": "ml_inference",
                    "confidence": 0.8,
                    "classification": result.get("classification", ""),
                }
        except Exception:
            pass

        # Default to ALPHA (coding agent)
        return {"agent": "ALPHA", "method": "default", "confidence": 0.5}

    # ------------------------------------------------------------------
    # Task Execution
    # ------------------------------------------------------------------

    def execute_task(self, task: dict) -> dict:
        """Execute a single task through the appropriate agent."""
        task_id = task.get("id", "unknown")
        title = task.get("title", "untitled")
        self._log(f"Executing: {title} [{task_id}]")

        # Classify
        routing = self.classify_task(task)
        agent = routing["agent"]
        self._log(f"  Routed to {agent} (method={routing['method']}, conf={routing['confidence']:.1f})")

        # Mark in-progress
        self._update_task_status(task_id, "in_progress")

        try:
            # Execute based on agent type
            # Modified: 2026-02-07T12:00:00Z | Author: COPILOT | Change: add COPILOT_CHAT routing
            if agent in ("ALPHA", "BETA"):
                result = self._execute_code_task(task, agent)
            elif agent == "GAMMA":
                result = self._execute_analysis_task(task)
            elif agent == "DELTA":
                result = self._execute_integration_task(task)
            elif agent == "COPILOT_CHAT":
                result = self._execute_copilot_chat_task(task)
            else:
                result = self._execute_complex_task(task)

            if result.get("success"):
                self._update_task_status(task_id, "completed")
                self.state["tasks_completed"] += 1
                # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Track completed ephemeral tasks in session set
                self.completed_ids.add(task_id)
                self._log(f"  Completed: {title}")
            else:
                self._update_task_status(task_id, "failed",
                                         error=result.get("error", "Unknown error"))
                self.state["tasks_failed"] += 1
                # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Track failed ephemeral tasks too to avoid re-execution
                self.completed_ids.add(task_id)
                self._log(f"  Failed: {title} - {result.get('error', '')}", "ERROR")

            # Record in history
            self.state["history"].append({
                "task_id": task_id,
                "title": title,
                "agent": agent,
                "success": result.get("success", False),
                "time": datetime.now(timezone.utc).isoformat(),
                "duration_s": result.get("duration_s", 0),
            })
            # Keep history trimmed
            if len(self.state["history"]) > 100:
                self.state["history"] = self.state["history"][-100:]

            self._save_state()
            return result

        except Exception as e:
            self._update_task_status(task_id, "failed", error=str(e))
            self.state["tasks_failed"] += 1
            self._log(f"  Exception: {e}", "ERROR")
            self._save_state()
            return {"success": False, "error": str(e)}

    def _execute_code_task(self, task: dict, agent: str) -> dict:
        """Execute a coding/testing task using SLATE ML inference with real output."""
        # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Fix test generation to write to tests/ dir, not source file
        start = time.time()
        title = task.get("title", "")
        desc = task.get("description", "")
        file_paths = task.get("file_paths", "")
        test_target = task.get("test_target", "")  # Where to write tests (if coverage task)

        try:
            # Read relevant files for context
            context = ""
            target_files = []
            if file_paths:
                for fp in str(file_paths).split(","):
                    fp = fp.strip()
                    full_path = self.workspace / fp
                    if full_path.exists() and full_path.is_file():
                        try:
                            content = full_path.read_text(encoding="utf-8", errors="replace")
                            context += f"\n--- {fp} ---\n{content[:4000]}\n"
                            target_files.append(fp)
                        except Exception:
                            pass

            # For coverage/test tasks, redirect output to test file instead of source
            is_test_task = bool(test_target) or task.get("source") == "coverage_gap"
            if is_test_task and test_target:
                # Override target_files to write to the test file
                target_files = [test_target]
                # Ensure tests directory exists
                test_path = self.workspace / test_target
                test_path.parent.mkdir(parents=True, exist_ok=True)
                if not test_path.exists():
                    test_path.write_text("", encoding="utf-8")

            # Generate solution using SLATE model with fallback
            task_type = "code_generation" if agent == "ALPHA" else "code_review"
            if is_test_task:
                system_prompt = (
                    "You are SLATE-CODER, an autonomous testing agent for the S.L.A.T.E. framework. "
                    "Write a COMPLETE test file with unit tests for the given module. "
                    "Use pytest conventions. Import the module under test. "
                    "Include at least 3 test functions covering key functionality. "
                    "Output ONLY the test file code in a python code block. "
                    "Include the Modified comment at the top."
                )
            else:
                system_prompt = (
                    "You are SLATE-CODER, an autonomous coding agent for the S.L.A.T.E. framework. "
                    "Analyze the task and provide a concrete implementation. "
                    "If code changes are needed, output the COMPLETE modified code for the file. "
                    "Include the Modified comment at the top. "
                    "Be precise, follow Python 3.11+ conventions, use type hints."
                )
            prompt = f"Task: {title}\nDescription: {desc}\n"
            if context:
                prompt += f"\nRelevant code:\n{context[:6000]}"

            self._log(f"  Running inference ({task_type}) on '{title[:50]}...'")
            result = self._infer_with_fallback(prompt, task_type=task_type,
                                               system=system_prompt,
                                               max_tokens=4096, temperature=0.3)

            response = result.get("response", "")
            tokens = result.get("tokens", 0)
            tok_per_sec = result.get("tok_per_sec", 0)

            self._log(f"  Inference complete: {tokens} tokens @ {tok_per_sec:.0f} tok/s")

            # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: apply code changes from inference
            # Attempt to apply code changes if the response contains code blocks
            applied_changes = []
            if target_files and agent == "ALPHA":
                applied_changes = self._apply_code_changes(response, target_files)
                if applied_changes:
                    self._log(f"  Applied {len(applied_changes)} code changes")

            # Log the response for audit
            log_file = LOG_DIR / f"task_{task.get('id', 'unknown')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            log_content = (
                f"# Task: {title}\n"
                f"**Agent**: {agent}\n"
                f"**Model**: {result.get('model', '?')}\n"
                f"**Tokens**: {tokens} @ {tok_per_sec:.0f} tok/s\n"
                f"**Time**: {datetime.now(timezone.utc).isoformat()}\n"
                f"**Changes Applied**: {len(applied_changes)}\n\n"
                f"## Response\n\n{response}\n"
            )
            if applied_changes:
                log_content += f"\n## Applied Changes\n\n"
                for ch in applied_changes:
                    log_content += f"- {ch}\n"
            try:
                log_file.write_text(log_content, encoding="utf-8")
            except Exception:
                pass

            elapsed = time.time() - start
            return {
                "success": True,
                "agent": agent,
                "response": response[:500],
                "model": result.get("model", ""),
                "tokens": tokens,
                "tok_per_sec": tok_per_sec,
                "duration_s": round(elapsed, 1),
                "changes_applied": len(applied_changes),
                "log_file": str(log_file.relative_to(self.workspace)) if log_file.exists() else None,
            }
        except Exception as e:
            return {"success": False, "error": str(e), "duration_s": round(time.time() - start, 1)}

    def _execute_analysis_task(self, task: dict) -> dict:
        """Execute an analysis/planning task."""
        # Modified: 2026-02-09T02:00:00Z | Author: COPILOT | Change: Use _infer_with_fallback
        start = time.time()
        try:
            result = self._infer_with_fallback(
                f"Analyze: {task.get('title', '')}. {task.get('description', '')}",
                task_type="planning",
                system="You are a senior architect. Provide a structured analysis with recommendations.",
                max_tokens=1024,
            )
            return {
                "success": True,
                "agent": "GAMMA",
                "response": result.get("response", "")[:500],
                "model": result.get("model", ""),
                "source": result.get("source", ""),
                "duration_s": round(time.time() - start, 1),
            }
        except Exception as e:
            return {"success": False, "error": str(e), "duration_s": round(time.time() - start, 1)}

    def _execute_integration_task(self, task: dict) -> dict:
        """Execute an integration task."""
        # Modified: 2026-02-09T02:00:00Z | Author: COPILOT | Change: Use _infer_with_fallback
        start = time.time()
        try:
            result = self._infer_with_fallback(
                f"Integration task: {task.get('title', '')}. {task.get('description', '')}",
                task_type="general",
                system="You are an integration specialist. Describe the integration steps needed.",
                max_tokens=1024,
            )
            return {
                "success": True,
                "agent": "DELTA",
                "response": result.get("response", "")[:500],
                "model": result.get("model", ""),
                "source": result.get("source", ""),
                "duration_s": round(time.time() - start, 1),
            }
        except Exception as e:
            return {"success": False, "error": str(e), "duration_s": round(time.time() - start, 1)}

    def _execute_complex_task(self, task: dict) -> dict:
        """Execute a complex multi-step task."""
        # Modified: 2026-02-09T02:00:00Z | Author: COPILOT | Change: Use _infer_with_fallback
        start = time.time()
        try:
            # Break down into steps
            plan_result = self._infer_with_fallback(
                f"Break this task into 3-5 concrete steps:\n{task.get('title', '')}\n{task.get('description', '')}",
                task_type="planning",
                system="Output a numbered list of steps. Be specific and actionable.",
                max_tokens=512, temperature=0.2,
            )
            return {
                "success": True,
                "agent": "COPILOT",
                "plan": plan_result.get("response", ""),
                "model": plan_result.get("model", ""),
                "source": plan_result.get("source", ""),
                "duration_s": round(time.time() - start, 1),
            }
        except Exception as e:
            return {"success": False, "error": str(e), "duration_s": round(time.time() - start, 1)}

    # Modified: 2026-02-07T12:00:00Z | Author: COPILOT | Change: COPILOT_CHAT bridge execution
    def _execute_copilot_chat_task(self, task: dict) -> dict:
        """Route task to the @slate VS Code chat participant via the agent bridge.

        This dispatches work to the COPILOT_CHAT agent, which enqueues tasks
        into the bridge queue file. The @slate TypeScript participant polls
        this queue and processes tasks using its full tool suite interactively.
        """
        start = time.time()
        try:
            from slate.copilot_agent_bridge import CopilotAgentBridge
            bridge = CopilotAgentBridge()

            task_id = task.get("id", f"auto_{int(time.time())}")
            title = task.get("title", "untitled")
            desc = task.get("description", title)

            # Enqueue to bridge for @slate participant pickup
            bridge.enqueue_task(
                task_id=task_id,
                title=title,
                description=desc,
                priority=task.get("priority", "medium"),
                agent="COPILOT_CHAT",
                source="autonomous_loop",
            )

            self._log(f"  Dispatched to @slate participant bridge: {title[:50]}")

            # Don't block — return immediately with pending status
            # The Copilot Runner or @slate participant will poll and complete it
            return {
                "success": True,
                "agent": "COPILOT_CHAT",
                "response": f"Task dispatched to @slate chat participant via bridge queue. Task ID: {task_id}",
                "bridge_task_id": task_id,
                "duration_s": round(time.time() - start, 1),
            }
        except Exception as e:
            return {"success": False, "error": str(e), "duration_s": round(time.time() - start, 1)}

    # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: code change application
    # Modified: 2026-02-09T11:35:00Z | Author: COPILOT | Change: Allow creating new test files and add safety for source files
    def _apply_code_changes(self, response: str, target_files: list[str]) -> list[str]:
        """Extract and apply code changes from inference response to files.

        Safety rules:
        - ActionGuard.validate_file_write() MUST approve every file before writing
        - For test files (tests/test_*.py): creates the file if it doesn't exist
        - For source files: only writes to files that already exist
        - Only writes Python files (.py)
        - Skips if response doesn't contain code blocks
        - Creates backup before overwriting existing files
        - Validates basic syntax before writing
        - BLOCKS writes to production files (slate/, agents/, configs, workflows, K8s)
        """
        # Modified: 2026-02-09T12:00:00Z | Author: COPILOT | Change: Add ActionGuard.validate_file_write gate to prevent AI overwriting production
        from slate.action_guard import get_guard
        guard = get_guard()
        applied = []

        # Extract code blocks from markdown-formatted response
        code_blocks = re.findall(r'```(?:python)?\s*\n(.*?)```', response, re.DOTALL)
        if not code_blocks:
            return []

        for i, fp in enumerate(target_files):
            if not fp.endswith(".py"):
                continue
            full_path = self.workspace / fp
            is_test_file = fp.startswith("tests/") and Path(fp).name.startswith("test_")

            # For non-test files, only write to existing files
            if not is_test_file and not full_path.exists():
                continue
            if i >= len(code_blocks):
                break

            code = code_blocks[i].strip()
            if len(code) < 20:  # Too short to be meaningful
                continue

            # Basic syntax check
            try:
                compile(code, fp, "exec")
            except SyntaxError:
                self._log(f"  Skipping {fp}: syntax error in generated code", "WARN")
                continue

            # Safety: don't write if it contains blocked patterns
            blocked = ["eval(", "exec(os", "rm -rf /", "base64.b64decode", "0.0.0.0"]
            if any(pat in code for pat in blocked):
                self._log(f"  Skipping {fp}: blocked pattern detected", "WARN")
                continue

            # CRITICAL: ActionGuard file write protection — blocks production files
            write_check = guard.validate_file_write(fp)
            if not write_check.allowed:
                self._log(f"  BLOCKED write to {fp}: {write_check.reason}", "WARN")
                continue

            # Create backup for existing files
            if full_path.exists():
                backup_dir = self.workspace / "slate_logs" / "backups"
                backup_dir.mkdir(parents=True, exist_ok=True)
                ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = backup_dir / f"{Path(fp).stem}_{ts}.py.bak"
                try:
                    backup_path.write_text(full_path.read_text(encoding="utf-8"), encoding="utf-8")
                except Exception:
                    pass

            # Ensure parent directory exists for new test files
            full_path.parent.mkdir(parents=True, exist_ok=True)

            # Write the code
            try:
                full_path.write_text(code, encoding="utf-8")
                action = "Created" if is_test_file and not full_path.exists() else "Updated"
                applied.append(f"{action} {fp} ({len(code)} chars)")
                self._log(f"  Wrote {len(code)} chars to {fp}")
            except Exception as e:
                self._log(f"  Failed to write {fp}: {e}", "ERROR")

        return applied

    def _update_task_status(self, task_id: str, status: str, error: str = ""):
        """Update task status in current_tasks.json."""
        if not TASK_FILE.exists():
            return
        try:
            data = json.loads(TASK_FILE.read_text(encoding="utf-8"))
            for task in data.get("tasks", []):
                if task.get("id") == task_id:
                    task["status"] = status
                    now = datetime.now(timezone.utc).isoformat()
                    if status == "in_progress":
                        task["started_at"] = now
                        task["started_by"] = "autonomous_loop"
                    elif status == "completed":
                        task["completed_at"] = now
                        task["completed_by"] = "autonomous_loop"
                    elif status == "failed":
                        task["failed_at"] = now
                        task["error"] = error
                    break
            data["last_updated"] = datetime.now(timezone.utc).isoformat()
            TASK_FILE.write_text(json.dumps(data, indent=2), encoding="utf-8")
        except Exception as e:
            self._log(f"Failed to update task status: {e}", "ERROR")

    # ------------------------------------------------------------------
    # Adaptive Learning
    # ------------------------------------------------------------------

    def adapt(self):
        """Adapt strategy based on recent results."""
        history = self.state.get("history", [])
        if len(history) < 5:
            return

        recent = history[-10:]
        success_rate = sum(1 for h in recent if h.get("success")) / len(recent)

        adaptations = []

        if success_rate < 0.5:
            adaptations.append({
                "type": "reduce_complexity",
                "reason": f"Low success rate: {success_rate:.0%}",
                "action": "Prioritize simpler tasks",
                "time": datetime.now(timezone.utc).isoformat(),
            })
            self._log(f"Adapting: low success rate ({success_rate:.0%}), prioritizing simpler tasks", "ADAPT")

        # Check for agent-specific failures
        agent_stats = {}
        for h in recent:
            agent = h.get("agent", "?")
            if agent not in agent_stats:
                agent_stats[agent] = {"success": 0, "fail": 0}
            if h.get("success"):
                agent_stats[agent]["success"] += 1
            else:
                agent_stats[agent]["fail"] += 1

        for agent, stats in agent_stats.items():
            total = stats["success"] + stats["fail"]
            if total >= 3 and stats["fail"] / total > 0.7:
                adaptations.append({
                    "type": "agent_issue",
                    "agent": agent,
                    "reason": f"{agent} failing {stats['fail']}/{total}",
                    "action": f"Re-route {agent} tasks to fallback",
                    "time": datetime.now(timezone.utc).isoformat(),
                })
                self._log(f"Adapting: {agent} has high failure rate, re-routing", "ADAPT")

        if adaptations:
            self.state["adaptations"].extend(adaptations)
            if len(self.state["adaptations"]) > 50:
                self.state["adaptations"] = self.state["adaptations"][-50:]
            self._save_state()

    # ------------------------------------------------------------------
    # Main Loop
    # ------------------------------------------------------------------

    def run(self, max_tasks: int = 50, stop_on_empty: bool = False,
            cycle_delay: float = 30.0):
        """Run the autonomous loop."""
        self.state["started_at"] = datetime.now(timezone.utc).isoformat()
        self._save_state()
        self._log(f"Autonomous loop started (max={max_tasks})")

        # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: warmup on loop start
        # Warm up models before starting task execution
        try:
            from slate.slate_warmup import SlateWarmup
            warmup = SlateWarmup()
            warmup.configure_ollama_env()
            result = warmup.preload_models()
            self._log(f"Warmup: {result.get('loaded', 0)} models preloaded to GPUs")
        except Exception as e:
            self._log(f"Warmup skipped: {e}", "WARN")

        tasks_executed = 0

        while tasks_executed < max_tasks:
            self.state["cycles"] += 1
            self.state["last_cycle"] = datetime.now(timezone.utc).isoformat()
            self._save_state()

            # Discover tasks
            tasks = self.discover_tasks()
            pending = [t for t in tasks if t.get("status") == "pending"]

            if not pending:
                if stop_on_empty:
                    self._log("No pending tasks, stopping (--stop-on-empty)")
                    break
                self._log(f"No pending tasks, waiting {cycle_delay}s...")
                time.sleep(cycle_delay)
                continue

            # Sort by priority
            priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
            pending.sort(key=lambda t: priority_order.get(t.get("priority", "medium"), 2))

            # Execute top task
            task = pending[0]
            self.execute_task(task)
            tasks_executed += 1

            # Adapt after every 5 tasks
            if tasks_executed % 5 == 0:
                self.adapt()

            # Modified: 2026-02-07T08:00:00Z | Author: COPILOT | Change: periodic model keep-alive
            # Refresh model keep-alive every 10 tasks to prevent GPU unloading
            if tasks_executed % 10 == 0:
                try:
                    from slate.slate_warmup import SlateWarmup
                    SlateWarmup().preload_models()
                    self._log("Refreshed model keep-alive")
                except Exception:
                    pass

            # Brief pause between tasks
            time.sleep(2)

        self._log(f"Autonomous loop finished: {tasks_executed} tasks executed")
        self.print_status()

    def run_single(self) -> dict:
        """Discover and execute a single task."""
        tasks = self.discover_tasks()
        pending = [t for t in tasks if t.get("status") == "pending"]
        if not pending:
            self._log("No pending tasks found")
            return {"success": False, "error": "No pending tasks"}

        # Pick highest priority
        priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        pending.sort(key=lambda t: priority_order.get(t.get("priority", "medium"), 2))
        return self.execute_task(pending[0])

    # ------------------------------------------------------------------
    # Status
    # ------------------------------------------------------------------

    def get_status(self) -> dict:
        """Get autonomous loop status."""
        # Get ML status
        ml_status = {}
        try:
            ml = self._get_ml()
            ml_status = {
                "ollama_running": ml.ollama.is_running(),
                "models": len(ml.ollama.list_models()),
                "loaded": len(ml.ollama.running_models()),
            }
        except Exception:
            ml_status = {"ollama_running": False}

        return {
            "started_at": self.state.get("started_at"),
            "cycles": self.state.get("cycles", 0),
            "last_cycle": self.state.get("last_cycle"),
            "tasks_discovered": self.state.get("tasks_discovered", 0),
            "tasks_completed": self.state.get("tasks_completed", 0),
            "tasks_failed": self.state.get("tasks_failed", 0),
            "success_rate": (
                self.state.get("tasks_completed", 0) /
                max(self.state.get("tasks_completed", 0) + self.state.get("tasks_failed", 0), 1)
            ),
            "adaptations": len(self.state.get("adaptations", [])),
            "recent_history": self.state.get("history", [])[-5:],
            "ml": ml_status,
        }

    def print_status(self):
        """Print human-readable status."""
        status = self.get_status()
        print("=" * 60)
        print("  SLATE Unified Autonomous Loop")
        print("=" * 60)
        print(f"\n  Started:    {status['started_at'] or 'Never'}")
        print(f"  Cycles:     {status['cycles']}")
        print(f"  Last Cycle: {status['last_cycle'] or 'N/A'}")
        print(f"\n  Discovered: {status['tasks_discovered']}")
        print(f"  Completed:  {status['tasks_completed']}")
        print(f"  Failed:     {status['tasks_failed']}")
        print(f"  Success:    {status['success_rate']:.0%}")
        print(f"  Adaptations: {status['adaptations']}")

        ml = status.get("ml", {})
        print(f"\n  Ollama:     {'Running' if ml.get('ollama_running') else 'Stopped'}")
        print(f"  Models:     {ml.get('models', 0)} available, {ml.get('loaded', 0)} loaded")

        if status["recent_history"]:
            print("\n  Recent Tasks:")
            for h in status["recent_history"]:
                icon = "+" if h.get("success") else "x"
                print(f"    [{icon}] {h.get('title', '?')[:50]} ({h.get('agent', '?')}, {h.get('duration_s', 0)}s)")

        print("\n" + "=" * 60)


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(description="SLATE Unified Autonomous Loop")
    parser.add_argument("--run", action="store_true", help="Run autonomous loop")
    parser.add_argument("--max", type=int, default=50, help="Max tasks to execute")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--discover", action="store_true", help="Discover tasks only")
    parser.add_argument("--single", action="store_true", help="Execute one task")
    parser.add_argument("--json", action="store_true", help="JSON output")
    parser.add_argument("--stop-on-empty", action="store_true", help="Stop when no tasks")
    args = parser.parse_args()

    loop = UnifiedAutonomousLoop()

    if args.run:
        loop.run(max_tasks=args.max, stop_on_empty=args.stop_on_empty)
    elif args.discover:
        tasks = loop.discover_tasks()
        pending = [t for t in tasks if t.get("status") == "pending"]
        print(f"Discovered {len(tasks)} tasks ({len(pending)} pending):")
        for t in pending[:15]:
            print(f"  [{t.get('priority', '?'):>6}] {t.get('title', '?')[:60]} ({t.get('source', '?')})")
    elif args.single:
        result = loop.run_single()
        print(json.dumps(result, indent=2, default=str))
    elif args.json:
        print(json.dumps(loop.get_status(), indent=2, default=str))
    else:
        loop.print_status()


if __name__ == "__main__":
    main()
